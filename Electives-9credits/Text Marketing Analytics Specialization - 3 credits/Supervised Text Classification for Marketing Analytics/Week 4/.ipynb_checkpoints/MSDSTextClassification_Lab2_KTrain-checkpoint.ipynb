{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBVrSZE47UNQ"
   },
   "source": [
    "# Text Classification, Lab 2: Building a model with K-Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vJvCM_x3buN"
   },
   "source": [
    "## ‚ö°Ô∏è Make a Copy\n",
    "\n",
    "Save a copy of this notebook in your Google Drive before continuing. Be sure to edit your own copy, not the original notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXuMewVjCBYy"
   },
   "source": [
    "## About this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ERHzPAZCE9t"
   },
   "source": [
    "The first part of this lab is identical to Lab 1, and you should be able to step through it easily.\n",
    "\n",
    "We continue now with the goal of building an inference model for predicting whether or not a document is about \"healthy living.\" We will use the K-Train library to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m45bF3QMOy1v"
   },
   "source": [
    "## About the final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_eBt6TiOdgc"
   },
   "source": [
    "\n",
    "Recall that you are working toward a final project. After completing this lab, you will want to go the extra mile and explore ways to tweak and improve your model. See the final project description for further details on what is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGSUArJ_ZYN3"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0MIZDrs0Ddx"
   },
   "source": [
    "We're going to be using Google's Tensorflow package: \n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "We're using an API wrapper for Tensorflow called ktrain. It's absolutely fabulous because it really abstracts the whole deep learning process into a workflow so easy, even a computational social scientist can do it:\n",
    "https://github.com/amaiya/ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16hoKG5dbeWs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.37.0.tar.gz (25.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (1.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (1.4.3)\n",
      "Requirement already satisfied: fastprogress>=0.1.21 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (1.0.3)\n",
      "Requirement already satisfied: requests in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (2.28.0)\n",
      "Requirement already satisfied: joblib in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (1.1.0)\n",
      "Requirement already satisfied: packaging in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (21.3)\n",
      "Collecting langdetect (from ktrain)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba (from ktrain)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cchardet (from ktrain)\n",
      "  Downloading cchardet-2.1.7-cp39-cp39-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (3.0.4)\n",
      "Collecting syntok>1.3.3 (from ktrain)\n",
      "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
      "Collecting tika (from ktrain)\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.17.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from ktrain) (4.20.1)\n",
      "Collecting sentencepiece (from ktrain)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras_bert>=0.86.0 (from ktrain)\n",
      "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting whoosh (from ktrain)\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from keras_bert>=0.86.0->ktrain) (1.21.3)\n",
      "Collecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->ktrain) (2022.1)\n",
      "Requirement already satisfied: regex>2016 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from syntok>1.3.3->ktrain) (2022.3.15)\n",
      "Requirement already satisfied: filelock in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.17.0->ktrain) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.17.0->ktrain) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.17.0->ktrain) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.17.0->ktrain) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from transformers>=4.17.0->ktrain) (4.64.0)\n",
      "Requirement already satisfied: six in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from langdetect->ktrain) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests->ktrain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests->ktrain) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests->ktrain) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from requests->ktrain) (2022.9.24)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->ktrain) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->ktrain) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from tika->ktrain) (61.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ryantalbot/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.17.0->ktrain) (4.1.1)\n",
      "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, jieba, langdetect, tika\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.37.0-py3-none-any.whl size=25320561 sha256=b017f8ac8af0b4a0b1c7213879e0f066e2e3acf78510a405cbed66fe63846d0c\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/97/0c/eb/7d6b71086f3c5ee0a5eb4d298943270250f57b95c05a8e5a2a\n",
      "  Building wheel for keras_bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33501 sha256=873e33c090af463cf8b54b6bcd602e59c2aed66b68c51350e40699f716707355\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/4e/26/24/14ecbc0166364db7f5500164b7d796263cf3cd10c57e892180\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12287 sha256=0f7f1069aec14d0b1128e33f93d0ddd781beb44cef132030a1ae504bf6d239b5\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/5e/d6/d1/c588c3b2b112c8f1173934995836ab2f2de8323cce99fa998f\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3944 sha256=7a63ab9df79d35c815725481c685e5370a79ef6f5b1056822eb0a800b13aef5e\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/cb/25/02/4bb438785ef9c10d07f6b3519f080b38917153fdac3108d738\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4655 sha256=18471d9fd6948989455c44d245b1ca13d2de6a648c1fc53c63cf4e21fa2148af\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/c1/df/15/a88cdf68ce687574649f65063a743123e1bee79932b6eea3b6\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14979 sha256=327399a62f51a08e65961429ae35c80c96c2b591dc12abb48c4ee7d5493ea739\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/b3/85/50/f232cac81ed1eb4dc20db31a9d1f4a8a1a8c696d4d27bff442\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6946 sha256=165ad55209b8117e9bebb14521c8bb330cee8bc35990eb65c429fef2045fac7e\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/f5/8c/9a/917bf72d493e084ca1706a02679185789c2715f50770d8c987\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4968 sha256=1334794a7f58058b283031e1dbeed457af2cd7822fefba576b49145128e9a851\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/20/36/25/efb605ab1742a179274a6f7cb113da1c6758f45e212b59bb4d\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18892 sha256=f72d3201895f7a2a5153029f5b3a8b56019e74d2f8dc32f8cb8e725939cecbbf\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/78/c1/84/b83a2fd6f1d63e136cba74bac4126bee3b8705eef6486635fd\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=c719902e0f81132979e98703d55bebba7426650e527436f2fbb420d5da8060a3\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=102542069245f45cd1a14f1e83b164a53f4e331b4c15eac4ea44a693d8e98bd6\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32625 sha256=dc241f7bdff28e2b4f7595ed1ea7536c45e99db363a1fbe4fcb913b420d3ff74\n",
      "  Stored in directory: /Users/ryantalbot/Library/Caches/pip/wheels/13/56/18/e752060632d32c39c9c4545e756dad281f8504dafcfac02b95\n",
      "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention jieba langdetect tika\n",
      "Installing collected packages: whoosh, sentencepiece, jieba, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, tika, keras-multi-head, keras-transformer, keras_bert, ktrain\n",
      "Successfully installed cchardet-2.1.7 jieba-0.42.1 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.37.0 langdetect-1.0.9 sentencepiece-0.1.99 syntok-1.4.4 tika-2.6.0 whoosh-2.7.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "  import ktrain\n",
    "except:\n",
    "  !pip install ktrain\n",
    "  os.kill(os.getpid(), 9)\n",
    "import ktrain\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaiVgZtNyRf2"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_ewzr0kyTXo"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3XGSpfy0xPv"
   },
   "source": [
    "## Set your google colab runtime to use GPU, a must for deep learning!\n",
    "\n",
    "Runtime > Change Runtime Type > GPU\n",
    "\n",
    "The following code snippet will show you GPU information for your runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0GWta-7ELIN"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLd-Qkl7Djha"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idE4-ojt0-dH"
   },
   "source": [
    "The data file should be in your Google Drive from Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7mhLblG6saq"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_json(\"drive/MyDrive/news_category_trainingdata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-NXTV--Y3ue"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quwyVcHh6P1s"
   },
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbCERZaXG0p-"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ7xO9Aa6iyU"
   },
   "source": [
    "Most machine learning tools in Python accept one field/column/string. So we have to merge our two text column. Let's separate it with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6VLEzaSuDyx"
   },
   "outputs": [],
   "source": [
    "reviews['combined_text'] = reviews['headline'] + ' ' + reviews['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKvESWUF5f4Y"
   },
   "source": [
    "The first thing we need to do is prepare the data. Specifically, we have a categorical column that we want to turn into a \"is this article healthy living?\" column. That is, when an article is about healthy living, it should have a 1, when it's anything else, it should be a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz1PoZOPriCk"
   },
   "outputs": [],
   "source": [
    "reviews[reviews['category'].str.contains(\"HEALTHY LIVING\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccC1GR85suZJ"
   },
   "outputs": [],
   "source": [
    "reviews['healthy'] = np.where((reviews['category'] == 'HEALTHY LIVING'), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aexMvzhKtyDN"
   },
   "outputs": [],
   "source": [
    "reviews['healthy'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S766s41yJVdf"
   },
   "source": [
    "## Balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4sGp-UdeBMh"
   },
   "source": [
    "To create a balanced data set that includes all of the health living articles, set sample_amount to the total number of those articles.\n",
    "\n",
    "In Lab 1, you balanced the data for the full set of healthy living articles. In the interest of getting through Lab 2 more quickly (in terms of training time for the model), we will use a smaller sample, of just 1000 articles per class. After completing the lab, consider increasing the sample size to see if you can get improvements on the model performance. Of course, be prepared for longer training times when you do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfLYND3fam_x"
   },
   "outputs": [],
   "source": [
    "# We have replaced the sample count with a smaller number in order to expedite\n",
    "# the completion of the lab. For your final project, you will want to use the\n",
    "# full balanced document set which is determined by this commented line:\n",
    "#sample_amount =  len(reviews[reviews[\"healthy\"] == 1]) # the total number of healthy living articles\n",
    "\n",
    "sample_amount = 1000\n",
    "\n",
    "healthy = reviews[reviews['healthy'] == 1].sample(n=sample_amount)\n",
    "not_healthy = reviews[reviews['healthy'] == 0].sample(n=sample_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B48IOQpGdsp1"
   },
   "outputs": [],
   "source": [
    "review_sample = pd.concat([healthy,not_healthy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHr4h7FYed_Z"
   },
   "outputs": [],
   "source": [
    "review_sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cljkg1U5aZ3_"
   },
   "source": [
    "# On to Lab 2: Test, Tune and Save Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KxLQ2lCprkx"
   },
   "source": [
    "Here, you will tune and train a predictor model for classifying healthy-living articles. After completing this lab, complete the Lab Quiz by entering your precision and recall values from the validation report for both the negative and positive classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUuRH7iYqPSX"
   },
   "outputs": [],
   "source": [
    "target_names = ['NOT HEALTHY LIVING','HEALTHY LIVING']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqj9ZZ4EQCAP"
   },
   "source": [
    "---\n",
    "\n",
    "### Experimenting with different transformers\n",
    "\n",
    "For purposes of this lab, we are using the **distilbert-base-uncased** transformer model. Other models you might try for your final project include:\n",
    "\n",
    " * roberta-base\n",
    " * bert-base-uncased\n",
    " * distilroberta-base\n",
    "\n",
    "See all the models here: https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "Some work, some dont, try at your own risk.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hknt0exnpGRt"
   },
   "outputs": [],
   "source": [
    "train, val, preprocess = ktrain.text.texts_from_df(\n",
    "    review_sample,\n",
    "    \"combined_text\",\n",
    "    label_columns=[\"healthy\"],\n",
    "    val_df=None,\n",
    "    max_features=20000,\n",
    "    maxlen=512,\n",
    "    val_pct=0.1,\n",
    "    ngram_range=1,\n",
    "    preprocess_mode=\"distilbert\",\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD_xu8e30bXp"
   },
   "outputs": [],
   "source": [
    "model = preprocess.get_classifier()\n",
    "learner = ktrain.get_learner(model, train_data=train, val_data=val, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlv9dyx20VH6"
   },
   "outputs": [],
   "source": [
    "learner.lr_find(max_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO5quriuQ80x"
   },
   "outputs": [],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKUljsZ5YTtQ"
   },
   "source": [
    "Now, use the tuned learner to train the best model.\n",
    "\n",
    "Here, we define a limit of 10 epochs, but in reality, this should stop much sooner due to early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX274hDU0j8R"
   },
   "outputs": [],
   "source": [
    "history=learner.autofit(\n",
    "    1e-4,\n",
    "    checkpoint_folder='checkpoint',\n",
    "    epochs=10,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOIwao0SpMsd"
   },
   "source": [
    "Get the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ti1VfWebw-a3"
   },
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9sqftuppYNb"
   },
   "source": [
    "Optionally, uncomment this code to save the predictor and reload it later. Note, the saved models can be quite large and may quickly use up space on your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqUostfKpStB"
   },
   "outputs": [],
   "source": [
    "#predictor.save(\"drive/MyDrive/MSDSTextClassification_Lab2.healthy_living\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv5DoNl-Ra8T"
   },
   "outputs": [],
   "source": [
    "validation = learner.validate(val_data=val, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G39ncMXqOKs"
   },
   "source": [
    "---\n",
    "\n",
    "## üßê Lab Quiz Questions 1-4\n",
    "\n",
    "Enter the following values from the validation report into the Lab Quiz:\n",
    "\n",
    " 1. precision for non healthy-living articles\n",
    " 2. recall for non healthy-living articles\n",
    " 3. precision for healthy-living articles\n",
    " 4. recall for healthy-living articles\n",
    "\n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A4jGZxtnlL5"
   },
   "source": [
    "Keep in mind that we've reduced the training set for the sake of expediency. For your final analysis and project, you should complete a run of the full data set. Pay attention to the impact of the input data on the performance of the final model (i.e. the validation scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g4KhsgzoQgr"
   },
   "source": [
    "# Inspecting the drivers of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLsIxS6houWD"
   },
   "source": [
    "No matter what the supervised machine learning model, you always want to peak under the hood to see what features are driving prediction. That is, what words sway the outcome of the prediction. It's harder to inspect a neural network. Because all of the layers of a neural network aren't really interpretable to the human eye. \n",
    "\n",
    "Currently, the best practice I've found is a little tool Explainable AI:\n",
    "https://alvinntnu.github.io/python-notes/nlp/ktrain-tutorial-explaining-predictions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6urVvKOqUHEl"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q git+https://github.com/amaiya/eli5@tfkeras_0_10_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB1_AaQBp35M"
   },
   "source": [
    "Let's go ahead and make a little set of test documents to check out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phek3FTqqAIl"
   },
   "outputs": [],
   "source": [
    "test_docs = [\n",
    "'Stress May Be Your Heart‚Äôs Worst Enemy Psychological stress activates the fear center in the brain, setting into motion a cascade of reactions that can lead to heart attacks and strokes.',\n",
    "'Exercising to Slim Down? Try Getting Bigger. It‚Äôs high time for women to reclaim the real strength behind exercise.',\n",
    "'What Are Your Food Resolutions for the New Year? Join us for the Eat Well Challenge starting in January.',\n",
    "'Why We All Need to Have More Fun. Prioritizing fun may feel impossible right now. But this four-step plan will help you rediscover how to feel more alive.',\n",
    "'Cuomo Will Not Be Prosecuted in Groping Case, Albany D.A. Says. The district attorney described the woman who said former Gov. Andrew Cuomo had groped her as ‚Äúcredible,‚Äù but added that proving her allegation would be difficult.',\n",
    "'A Film Captures Jewish Life in a Polish Town Before the Nazis Arrived. A documentary based on a home movie shot by an American in 1938 provides a look at the vibrancy of a Jewish community in Europe just before the Holocaust.' \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQbBKDiPb9FO"
   },
   "outputs": [],
   "source": [
    "for i, text in enumerate(test_docs):\n",
    "  probs = predictor.predict(text, return_proba=True)\n",
    "  print(\"---------------------------\")\n",
    "  print('The probability this is healthy is %s' % probs[1])\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hha7-WCOr0RN"
   },
   "source": [
    "*These* are pretty obvious examples, but it works exactly as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVpTNeKqXhov"
   },
   "outputs": [],
   "source": [
    "predictor.explain('Diversity is the key to a healthy society. Here is what we need to do to make america a more equitable place to live for all.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWgUdJuxsJ1r"
   },
   "source": [
    "But you can see, this algorithm is far from perfect. Here you can see that it's probably got too high of an emphasis on the word \"healthy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o9GW2qAsfCA"
   },
   "source": [
    "So what would I do next? Well, given that it's over reacting to worrds like health and equitable, I'd try introducing more negative examples into the data, times where healthy is used outside of health and wellness news. We can do this by changing our sample from 50/50 to something like 20/80, but of course, the more documents we process, the longer this model is going to take to make!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
