{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e9YmokLnvF-y",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21c5038399aa73c841ec3f678e8a204c",
     "grade": false,
     "grade_id": "cell-476ea20a1864a2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Language Models Lab Assignment\n",
    "## Introduction to Language Models\n",
    "Large Language Models (LLMs) like GPT-4, Claude, and LLaMA have revolutionized natural language processing. While these models have billions of parameters and are trained on vast datasets, they share fundamental architectural similarities with the model you'll build in this assignment. Most modern LLMs are based on decoder-only transformer architectures that generate text auto-regressively.\n",
    "\n",
    "**Auto-regressive language modeling** means the model predicts the next token in a sequence based on all previous tokens. This approach allows the model to generate coherent text by repeatedly predicting and sampling new tokens, conditioned on the tokens it has already generated. The \"language modeling\" objective—predicting the next word given previous words—allows these models to learn syntax, semantics, factual knowledge, and even reasoning capabilities, all from the patterns in text.\n",
    "\n",
    "In this assignment, you'll implement a word-level language model using the tiny Shakespeare dataset, which contains the complete works of William Shakespeare (~1MB of text). While small by modern standards, this dataset provides enough structure and patterns for our model to learn meaningful representations and generate Shakespeare-like text. The language, featuring archaic words and poetic structure, presents an interesting stylistic target for our model.\n",
    "Through building this transformer-based architecture and training it to predict the next word in Shakespeare's texts, you'll gain insight into the core mechanisms that power today's most advanced language models—just at a much smaller scale.\n",
    "\n",
    "In this assignment, you will build and train a word-level language model using PyTorch. You'll start by preprocessing text data, constructing a vocabulary, and encoding sentences into numerical representations. Using this processed data, you will implement key components of a transformer-based language model, including positional encoding, self-attention mechanisms, and feed-forward networks.\n",
    "\n",
    "By the end, you will have hands-on experience with building and training neural language models, helping you understand how modern language models process and generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CFayOXdObhhg",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3736c7f1e21e542eec3dc84541d18386",
     "grade": false,
     "grade_id": "cell-a77e80476a25befd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1 : Data Preparation\n",
    "\n",
    "In this section, you'll set up the data processing pipeline to transform raw text into a format suitable for training a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "jdj-cn8H1Wke",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56886620d0fed2c59f30727ba4d5d97",
     "grade": false,
     "grade_id": "cell-280b1971e49da81d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "012a9dfa-2e5c-426f-f52a-6a7f0d1f3b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dataset: 202651\n",
      "Vocabulary size: 25670\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "\n",
    "# Example: Tiny Shakespeare (still somewhat weird for word-level, but let's demonstrate)\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text_data = requests.get(url).text\n",
    "\n",
    "# 1. Split into words (naive approach)\n",
    "words = text_data.split()\n",
    "print(\"Number of words in dataset:\", len(words))\n",
    "\n",
    "# 2. Build the vocabulary from the set of unique words\n",
    "unique_words = sorted(list(set(words)))\n",
    "vocab_size = len(unique_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B1x3d4UVborU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "464543895669723d619cb526470a7bb2",
     "grade": false,
     "grade_id": "cell-68e0d30354d38048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The first step in building a language model is to process the raw text data. This includes tokenizing the text (splitting it into words) and building a vocabulary (the set of unique words in the dataset). For this assignment, we're using a simplified tokenization approach by splitting on whitespace.\n",
    "The Shakespeare dataset contains approximately 200,000 words with a vocabulary of about 25,000 unique words. This rich vocabulary includes archaic terms, character names, and Shakespearean expressions that give the text its distinctive style. Our model will learn to predict the next word in this unique literary style, eventually generating text that mimics Shakespeare's writing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "id": "Eb2Y5mA7rzii",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f9bb19f26dcd3177d2f9c3da3aabdc",
     "grade": false,
     "grade_id": "cell-c4e0d681d095d6b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create two dictionaries - one to hold words to indices and another to hold indices to words\n",
    "# word_to_idx = # TODO\n",
    "# idx_to_word = # TODO\n",
    "\n",
    "# your code here\n",
    "# raise NotImplementedError\n",
    "\n",
    "\n",
    "word_to_idx = {w: i for i, w in enumerate(unique_words)}\n",
    "idx_to_word = {i: w for i, w in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "u1DCMGIWc_mq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f4f60a84af151785a267144fafbd878",
     "grade": true,
     "grade_id": "cell-44b07b7e9501bf60",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "147a9b50-9ada-43b0-a72a-8a1bc52479b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Word mappings test passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_word_mappings(student_word_to_idx, student_idx_to_word, unique_words):\n",
    "    \"\"\"\n",
    "    Tests if the word-to-index and index-to-word mappings are correctly implemented.\n",
    "    \"\"\"\n",
    "    # Check if dictionaries have correct size\n",
    "    assert len(student_word_to_idx) == len(unique_words), f\"word_to_idx should have {len(unique_words)} entries\"\n",
    "    assert len(student_idx_to_word) == len(unique_words), f\"idx_to_word should have {len(unique_words)} entries\"\n",
    "\n",
    "    # Check if all words are mapped correctly\n",
    "    for i, word in enumerate(sorted(unique_words)):\n",
    "        assert word in student_word_to_idx, f\"Word '{word}' is missing from word_to_idx\"\n",
    "        assert student_word_to_idx[word] == i, f\"Word '{word}' should map to index {i}\"\n",
    "        assert i in student_idx_to_word, f\"Index {i} is missing from idx_to_word\"\n",
    "        assert student_idx_to_word[i] == word, f\"Index {i} should map to word '{word}'\"\n",
    "\n",
    "    # Check if mappings are inverses\n",
    "    for word, idx in student_word_to_idx.items():\n",
    "        assert student_idx_to_word[idx] == word, f\"Mappings are not inverses for word '{word}'\"\n",
    "\n",
    "    print(\"✓ Word mappings test passed!\")\n",
    "    return True\n",
    "\n",
    "test_word_mappings(word_to_idx, idx_to_word, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "id": "SkhzJmqwr0R6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3878e83ea8368f334da9b61c1e421f50",
     "grade": false,
     "grade_id": "cell-17f53ab1a3ae89f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode a sequence of words by converting them into indices\n",
    "def encode_word_sequence(seq):\n",
    "    # TODO : Return a list of indices for the given sequence\n",
    "    # your code here\n",
    "    tokens = seq.split() if isinstance(seq, str) else list(seq)\n",
    "    return [word_to_idx[w] for w in tokens]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n",
    "\n",
    "def decode_word_sequence(seq_ids):\n",
    "    # TODO : Return a string given a list of indices\n",
    "\n",
    "    # your code here\n",
    "    return \" \".join(idx_to_word[i] for i in seq_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "Nh31EtqdscDa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8651039d5b0243e90b7a0f3a3f566538",
     "grade": true,
     "grade_id": "cell-d94263c07bf09305",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "6d094117-2196-4cc9-845e-886c88de414a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing encode_word_sequence...\n",
      "Testing decode_word_sequence...\n",
      "All tests passed! The encoding and decoding functions are implemented correctly.\n"
     ]
    }
   ],
   "source": [
    "def test_encoding_decoding_functions():\n",
    "    \"\"\"\n",
    "    Test the implementation of encode_word_sequence and decode_word_sequence functions.\n",
    "    Run this after defining word_to_idx and idx_to_word dictionaries.\n",
    "    \"\"\"\n",
    "    # Test encoding\n",
    "    print(\"Testing encode_word_sequence...\")\n",
    "\n",
    "    # Test with single word\n",
    "    test_seq1 = [\"the\"]\n",
    "    encoded1 = encode_word_sequence(test_seq1)\n",
    "    assert isinstance(encoded1, list), \"encode_word_sequence should return a list\"\n",
    "    assert len(encoded1) == 1, \"Encoded sequence length should match input sequence length\"\n",
    "    assert encoded1[0] == word_to_idx[\"the\"], f\"Wrong index for 'the', expected {word_to_idx['the']}, got {encoded1[0]}\"\n",
    "\n",
    "    # Test with multiple words\n",
    "    test_seq2 = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\"]\n",
    "    encoded2 = encode_word_sequence(test_seq2)\n",
    "    assert len(encoded2) == 6, \"Encoded sequence length should match input sequence length\"\n",
    "    expected2 = [word_to_idx[\"to\"], word_to_idx[\"be\"], word_to_idx[\"or\"],\n",
    "                word_to_idx[\"not\"], word_to_idx[\"to\"], word_to_idx[\"be\"]]\n",
    "    assert encoded2 == expected2, f\"Wrong encoding, expected {expected2}, got {encoded2}\"\n",
    "\n",
    "    # Test decoding\n",
    "    print(\"Testing decode_word_sequence...\")\n",
    "\n",
    "    # Test with single index\n",
    "    test_idx1 = [word_to_idx[\"the\"]]\n",
    "    decoded1 = decode_word_sequence(test_idx1)\n",
    "    assert isinstance(decoded1, str), \"decode_word_sequence should return a string\"\n",
    "    assert decoded1 == \"the\", f\"Wrong decoding, expected 'the', got '{decoded1}'\"\n",
    "\n",
    "    # Test with multiple indices\n",
    "    test_idx2 = [word_to_idx[w] for w in [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\"]]\n",
    "    decoded2 = decode_word_sequence(test_idx2)\n",
    "    expected_str = \"to be or not to be\"\n",
    "    assert decoded2 == expected_str, f\"Wrong decoding, expected '{expected_str}', got '{decoded2}'\"\n",
    "\n",
    "    # Test round trip\n",
    "    test_seq3 = [\"shall\", \"I\", \"compare\", \"thee\", \"to\", \"a\", \"summer's\", \"day\"]\n",
    "    round_trip = decode_word_sequence(encode_word_sequence(test_seq3))\n",
    "    expected_rt = \"shall I compare thee to a summer's day\"\n",
    "    assert round_trip == expected_rt, f\"Round trip failed, expected '{expected_rt}', got '{round_trip}'\"\n",
    "\n",
    "    print(\"All tests passed! The encoding and decoding functions are implemented correctly.\")\n",
    "\n",
    "test_encoding_decoding_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "ybv5GUEir2dv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8833148b015b7644d767d1954949981d",
     "grade": false,
     "grade_id": "cell-341a2c8ce1207280",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a017b402-e360-42b8-c055-2b94a0fcef31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset size: torch.Size([202651])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode_word_sequence(words), dtype=torch.long)\n",
    "print(\"Encoded dataset size:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cFG0o79Th4_f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d88f5994a4f212d16605705cb78851d9",
     "grade": false,
     "grade_id": "cell-ab7d673364dc3276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will use a simple data split for the purpose of this lab. The validation set will have 10% of the data. Normally, we would also want an external test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rVdJ5fGqr8_d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ba75d56d88bc1190346451f7deb330",
     "grade": false,
     "grade_id": "cell-e0bd6c69af71007c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "split_idx = int(0.9 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6JpfKZb4iD3C",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f948c545efbdd6e672fb490a2ba27cd",
     "grade": false,
     "grade_id": "cell-6e198493a0ee55c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Training Paradigm\n",
    "\n",
    "An important aspect of autoregressive generative models is that they should be able to generate the next word given an initial sequence of words. When you type out a prompt in ChatGPT, you are giving it this initial sequence of words from which the model can begin generating text. To set this up correctly for training, we will use a block size of 32 and a batch size of 16. The block determines how many tokens are contained in a sequence and the batch determines the number of sequences to process at a time.\n",
    "\n",
    "Let's use random starting positions and sample from the dataset repeatedly. Create two tensors 'x' and 'y'. 'x' is a set of words in the dataset that starts from a random index and contains the subsequent 32 tokens. 'y' is the same set shifted by one index. Each word in 'x' will have a shifted target value in 'y'. In other words, given an input token, the goal is to predict the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "l3oCpopRsA5d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134ff30f86cb40ea2971ff2f5b1a09f0",
     "grade": false,
     "grade_id": "cell-b3e71dd96e2cc1e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "281bbecd-56b6-4d62-cfc9-e19abf10e266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([16, 32]) y shape: torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "block_size = 32  # how many words per context\n",
    "batch_size = 16  # sequences per batch\n",
    "\n",
    "def get_batch(split='train'):\n",
    "    dataset = train_data if split == 'train' else val_data\n",
    "    # random starting positions\n",
    "    idxs = torch.randint(len(dataset) - block_size - 1, (batch_size,))\n",
    "\n",
    "\n",
    "    # TODO\n",
    "    # x =\n",
    "    # y =\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    x, y = [], []\n",
    "    for start in idxs.tolist():\n",
    "        seq = dataset[start:start + block_size + 1]\n",
    "        if not isinstance(seq, torch.Tensor):\n",
    "            seq = torch.tensor(seq, dtype=torch.long)\n",
    "        x.append(seq[:block_size])\n",
    "        y.append(seq[1:block_size + 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n",
    "\n",
    "    # Convert the lists to tensors by stacking them\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"x shape:\", xb.shape, \"y shape:\", yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "7AmSk1hzs6Oe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5c5807f430e331c3de8298723b30fae",
     "grade": true,
     "grade_id": "cell-7f8f64aea126c86b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ac0c5151-1a6d-4743-bc10-45167022a135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_batch function...\n",
      "All tests passed! The get_batch function is implemented correctly.\n"
     ]
    }
   ],
   "source": [
    "def test_get_batch_function():\n",
    "    \"\"\"\n",
    "    Test the implementation of get_batch function.\n",
    "    Run this after defining block_size, batch_size, train_data, and val_data.\n",
    "    \"\"\"\n",
    "    print(\"Testing get_batch function...\")\n",
    "\n",
    "    # Test with train data\n",
    "    x_train, y_train = get_batch('train')\n",
    "\n",
    "    # Check shapes\n",
    "    assert x_train.shape == (batch_size, block_size), f\"x_train shape should be ({batch_size}, {block_size}), got {x_train.shape}\"\n",
    "    assert y_train.shape == (batch_size, block_size), f\"y_train shape should be ({batch_size}, {block_size}), got {y_train.shape}\"\n",
    "\n",
    "    # Check target is shifted by one position (next token prediction)\n",
    "    for i in range(batch_size):\n",
    "        assert torch.equal(x_train[i, 1:], y_train[i, :-1]), f\"Target sequence should be shifted by one position for batch {i}\"\n",
    "\n",
    "    # Test with validation data\n",
    "    x_val, y_val = get_batch('val')\n",
    "\n",
    "    # Check shapes\n",
    "    assert x_val.shape == (batch_size, block_size), f\"x_val shape should be ({batch_size}, {block_size}), got {x_val.shape}\"\n",
    "    assert y_val.shape == (batch_size, block_size), f\"y_val shape should be ({batch_size}, {block_size}), got {y_val.shape}\"\n",
    "\n",
    "    # Check target is shifted by one position (next token prediction)\n",
    "    for i in range(batch_size):\n",
    "        assert torch.equal(x_val[i, 1:], y_val[i, :-1]), f\"Target sequence should be shifted by one position for batch {i}\"\n",
    "\n",
    "    # Check that using different splits gives different data\n",
    "    # Note: This is probabilistic but highly likely with a large dataset\n",
    "    assert not torch.equal(x_train, x_val), \"Training and validation batches should be different\"\n",
    "\n",
    "    # Check that each batch sample starts at the position given by the random index\n",
    "    # We need to recreate the random indices to verify this\n",
    "    torch.manual_seed(0)  # Set seed for reproducibility\n",
    "    idxs = torch.randint(len(train_data) - block_size - 1, (batch_size,))\n",
    "    torch.manual_seed(0)  # Reset seed to get the same indices\n",
    "    x_seeded, _ = get_batch('train')\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        expected_start = train_data[idxs[i]]\n",
    "        assert x_seeded[i, 0] == expected_start, f\"Batch sample {i} should start with token at index {idxs[i]}\"\n",
    "\n",
    "    print(\"All tests passed! The get_batch function is implemented correctly.\")\n",
    "\n",
    "test_get_batch_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "miwjXpdrfLtr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12c0d35591286bb9769b00e8ca33eb8e",
     "grade": false,
     "grade_id": "cell-608f1e1b6893747f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1beb22b8-c749-458a-8a84-df8af316c801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: LAURENCE: Stay, then; I'll go alone. Fear comes upon me: O, much I fear some ill unlucky thing. BALTHASAR: As I did sleep under this yew-tree here, I dreamt my master and\n",
      "Target 0: Stay, then; I'll go alone. Fear comes upon me: O, much I fear some ill unlucky thing. BALTHASAR: As I did sleep under this yew-tree here, I dreamt my master and another\n",
      "\n",
      "---------------\n",
      "Example 1: bestowed her on her own lamentation, which she yet wears for his sake; and he, a marble to her tears, is washed with them, but relents not. ISABELLA: What a merit were\n",
      "Target 1: her on her own lamentation, which she yet wears for his sake; and he, a marble to her tears, is washed with them, but relents not. ISABELLA: What a merit were it\n",
      "\n",
      "---------------\n",
      "Example 2: for a commodity of brown paper and old ginger, ninescore and seventeen pounds; of which he made five marks, ready money: marry, then ginger was not much in request, for the old\n",
      "Target 2: a commodity of brown paper and old ginger, ninescore and seventeen pounds; of which he made five marks, ready money: marry, then ginger was not much in request, for the old women\n",
      "\n",
      "---------------\n",
      "Example 3: my father had left me no more! For all the rest is held at such a rate As brings a thousand-fold more care to keep Than in possession and jot of pleasure.\n",
      "Target 3: father had left me no more! For all the rest is held at such a rate As brings a thousand-fold more care to keep Than in possession and jot of pleasure. Ah,\n",
      "\n",
      "---------------\n",
      "Example 4: strange lunacy. O noble lord, bethink thee of thy birth, Call home thy ancient thoughts from banishment And banish hence these abject lowly dreams. Look how thy servants do attend on thee,\n",
      "Target 4: lunacy. O noble lord, bethink thee of thy birth, Call home thy ancient thoughts from banishment And banish hence these abject lowly dreams. Look how thy servants do attend on thee, Each\n",
      "\n",
      "---------------\n",
      "Example 5: shall run A cold and drowsy humour, for no pulse Shall keep his native progress, but surcease: No warmth, no breath, shall testify thou livest; The roses in thy lips and cheeks\n",
      "Target 5: run A cold and drowsy humour, for no pulse Shall keep his native progress, but surcease: No warmth, no breath, shall testify thou livest; The roses in thy lips and cheeks shall\n",
      "\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Let us now look at some examples from the dataset\n",
    "example_batch = [decode_word_sequence(x.tolist()) for x in xb]\n",
    "for i, ex in enumerate(example_batch):\n",
    "    print(f\"Example {i}: {ex}\")\n",
    "    print(f\"Target {i}: {decode_word_sequence(yb[i].tolist())}\\n\")\n",
    "    print(\"---------------\")\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DmgRymJlgxHa",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45b475807340e8b14d301ec23fc50edc",
     "grade": false,
     "grade_id": "cell-e560609160337732",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Implementing the Self-Attention Mechanism\n",
    "\n",
    "The Head class implements a self-attention mechanism that allows the model to focus on different parts of the input sequence when making predictions. The \"causal\" part means it only allows a token to attend to itself and previous tokens (not future ones), which is essential for autoregressive language modeling.\n",
    "\n",
    "1. **Contextual Understanding**: The attention mechanism lets the model relate different positions in a sequence, capturing dependencies between words regardless of their distance.\n",
    "2. **Parallel Processing**: Unlike RNNs, transformers process all tokens in parallel, making them much faster to train.\n",
    "3. **Causal Masking**: The triangular mask ensures the model only looks at past tokens when predicting the next one, which is essential for autoregressive generation.\n",
    "4. **Flexible Focus**: Each token can \"pay attention\" to relevant parts of the input sequence, rather than treating all context equally.\n",
    "5. **Scaling Dot-Product**: The scaled dot-product attention mechanism is efficient and helps stabilize training by preventing extremely large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "id": "5L3Ssk9ssEUu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33ef3d6bdca268ce750718a491bfef95",
     "grade": false,
     "grade_id": "cell-bcaf7e420240246f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, embed_dim, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear projection for keys : transforms embeddings to attention space\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "\n",
    "        # self.query =\n",
    "        # self.value =\n",
    "\n",
    "        # your code here\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Create a lower triangular matrix for causal masking (each token can only attend to itself and previous tokens)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get dimensions: Batch size, Sequence length, Embedding dimension\n",
    "        B, T, C = x.shape # shape: (B, T, head_size)\n",
    "\n",
    "        # Project input embeddings to key space\n",
    "        k = self.key(x) # shape: (B, T, head_size)\n",
    "\n",
    "        # Project input embeddings to query space\n",
    "        q = self.query(x) # shape: (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores: dot product of queries and keys, scaled by sqrt(head dimension)\n",
    "        # The scaling prevents values from exploding when dimension is large\n",
    "        weights = q @ k.transpose(-2, -1) / math.sqrt(C)\n",
    "\n",
    "        # Apply causal mask: ensure tokens can only attend to previous tokens (and themselves)\n",
    "        mask = self.tril[:T, :T] # Get appropriate sized mask for current sequence length\n",
    "        weights = weights.masked_fill(mask == 0, float('-inf')) # Set masked positions to -infinity\n",
    "\n",
    "        # Apply softmax to get attention probabilities\n",
    "        att = torch.softmax(weights, dim=-1) # shape: (B, T, T)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        # Project input embeddings to value space\n",
    "        v = self.value(x) # shape: (B, T, head_size)\n",
    "\n",
    "        # Compute weighted sum of values based on attention weights\n",
    "        out = att @ v # shape: (B, T, head_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "_jivXLgFtRUB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d64b12d8bdd9429305d81df2ba83b8b9",
     "grade": true,
     "grade_id": "cell-66f77c50b37ccedf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0ebec8e8-2bec-40fe-e98c-10c660f60524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Head class implementation...\n",
      "All tests passed! The Head class is implemented correctly.\n"
     ]
    }
   ],
   "source": [
    "def test_head_implementation():\n",
    "    \"\"\"\n",
    "    Test the implementation of the attention head.\n",
    "    This test verifies that students have correctly implemented the query and value projections.\n",
    "    \"\"\"\n",
    "    print(\"Testing Head class implementation...\")\n",
    "\n",
    "    # Set up test parameters\n",
    "    head_size = 8\n",
    "    embed_dim = 16\n",
    "    block_size = 10\n",
    "    batch_size = 4\n",
    "    seq_len = 6\n",
    "\n",
    "    # Create a test input tensor\n",
    "    x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # Initialize the head\n",
    "    head = Head(head_size, embed_dim, block_size)\n",
    "\n",
    "    # Check that query and value projections exist\n",
    "    assert hasattr(head, 'query'), \"Head class missing 'query' attribute\"\n",
    "    assert hasattr(head, 'value'), \"Head class missing 'value' attribute\"\n",
    "\n",
    "    # Check that they are nn.Linear instances\n",
    "    assert isinstance(head.query, nn.Linear), \"'query' should be an instance of nn.Linear\"\n",
    "    assert isinstance(head.value, nn.Linear), \"'value' should be an instance of nn.Linear\"\n",
    "\n",
    "    # Check the shapes of the weight matrices\n",
    "    assert head.query.weight.shape == (head_size, embed_dim), f\"Query weight matrix should be shape ({head_size}, {embed_dim})\"\n",
    "    assert head.value.weight.shape == (head_size, embed_dim), f\"Value weight matrix should be shape ({head_size}, {embed_dim})\"\n",
    "\n",
    "    # Check that bias is False\n",
    "    assert head.query.bias is None, \"Query projection should have bias=False\"\n",
    "    assert head.value.bias is None, \"Value projection should have bias=False\"\n",
    "\n",
    "    # Test forward pass\n",
    "    try:\n",
    "        output = head(x)\n",
    "        assert output.shape == (batch_size, seq_len, head_size), f\"Output shape should be ({batch_size}, {seq_len}, {head_size})\"\n",
    "\n",
    "        # Extra verification: ensure forward pass uses the query and value projections\n",
    "        # We can do this by temporarily replacing them with dummy functions that raise exceptions\n",
    "        original_query = head.query\n",
    "        original_value = head.value\n",
    "\n",
    "        # Create dummy functions\n",
    "        class DummyModule(nn.Module):\n",
    "            def forward(self, x):\n",
    "                raise RuntimeError(\"This method should be called\")\n",
    "\n",
    "        # Test query projection is used\n",
    "        try:\n",
    "            head.query = DummyModule()\n",
    "            try:\n",
    "                head(x)\n",
    "                assert False, \"Forward pass didn't use query projection\"\n",
    "            except RuntimeError:\n",
    "                pass  # Expected error\n",
    "        finally:\n",
    "            head.query = original_query\n",
    "\n",
    "        # Test value projection is used\n",
    "        try:\n",
    "            head.value = DummyModule()\n",
    "            try:\n",
    "                head(x)\n",
    "                assert False, \"Forward pass didn't use value projection\"\n",
    "            except RuntimeError:\n",
    "                pass  # Expected error\n",
    "        finally:\n",
    "            head.value = original_value\n",
    "\n",
    "    except Exception as e:\n",
    "        assert False, f\"Forward pass failed: {str(e)}\"\n",
    "\n",
    "    print(\"All tests passed! The Head class is implemented correctly.\")\n",
    "\n",
    "test_head_implementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xxbitF_Tj6eK",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e61609726c03717cd95db7d5ff86873",
     "grade": false,
     "grade_id": "cell-917e2a09f2d28546",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have defined a single attnetion head, let's make a class for multi-head attention. Let's also define a simple feedforward class. You don't have to modify any of this code. Read through it to understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "98RMcmd23L5j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0337e253b77e513cb02d4bdbbbf009ef",
     "grade": false,
     "grade_id": "cell-5ee42336d4b656e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, embed_dim, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(head_size, embed_dim, block_size, dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(num_heads * head_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rsbC2b_b3OHv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fec2253d2a69bc402145b3cbb126fb07",
     "grade": false,
     "grade_id": "cell-b8760a384b1ed6c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffw_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffw_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffw_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BQSH3ckqkrl9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2179669ec2f94873109fb7d90eb58576",
     "grade": false,
     "grade_id": "cell-71bbeb3daf31ad1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The Transformer Block\n",
    "\n",
    "The TransformerBlock class encapsulates the core structure found in transformer models: a multi-head attention layer followed by a feed-forward network, each preceded by layer normalization and accompanied by residual connections. This architecture allows for effective processing of sequential data by combining the contextual awareness of attention with the representational power of feed-forward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "o46R3wEm3QYA",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d098a5b75bbb201986d0c23d27aeea05",
     "grade": false,
     "grade_id": "cell-437b645fd142a786",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, block_size, ffw_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the size of each attention head\n",
    "        head_size = embed_dim // num_heads\n",
    "\n",
    "        # Layer normalization before the attention block\n",
    "        # Helps stabilize and accelerate training by normalizing inputs\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(num_heads, head_size, embed_dim, block_size, dropout)\n",
    "        self.ffw = FeedForward(embed_dim, ffw_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection around the attention block\n",
    "        # First normalize the input, then apply attention, then add back the original input\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "\n",
    "        # Residual connection around the feed-forward block\n",
    "        # First normalize the input, then apply feed-forward, then add back the intermediate result\n",
    "        x = x + self.ffw(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9Za_CDEPl1h9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d761c89cee2f943cd75c2a062cd226b",
     "grade": false,
     "grade_id": "cell-21dba73b57f57cc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Understanding a Basic GPT Language Model Architecture\n",
    "\n",
    "The GPT class defines the entire architecture of the language model. It combines token embeddings, positional embeddings, multiple transformer blocks, and a final projection layer to create a generative model that predicts the next token in a sequence. It also includes a text generation method that can produce new content autoregressively.\n",
    "\n",
    "1. **Complete Language Model**: This class brings together all components needed for a generative language model - embeddings, attention, transformations, and prediction.\n",
    "2. **Positional Information**: Since transformers process all tokens in parallel, the positional embeddings are crucial to retain sequence order information.\n",
    "3. **Multi-Layer Processing**: The stacked transformer blocks allow the model to learn increasingly abstract representations of the text.\n",
    "4. **Training and Generation**: The class handles both the training process (via the loss calculation) and the generation process (via the generate method).\n",
    "5. **Autoregressive Generation**: The generate method implements autoregressive text generation - each new token is based on all previously generated tokens.\n",
    "6. **Context Management**: The model handles context length limitations by truncating inputs to the maximum block size when needed.\n",
    "7. **Token Sampling**: Rather than always selecting the most probable next token, the model samples from the probability distribution, leading to more diverse text generation.\n",
    "\n",
    "This GPT architecture has become the foundation for many state-of-the-art language models. Its ability to learn from large corpora of text and generate coherent, contextually relevant content has led to its widespread adoption in many natural language processing applications, from chatbots to content creation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "id": "ECDYBEiA3S5-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1025172454ce6299d8147029035940d",
     "grade": false,
     "grade_id": "cell-4b275dc717bc7c73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, n_layers, num_heads, ffw_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Embedding layer that converts token IDs to vectors\n",
    "\n",
    "        # self.token_embed = # TODO\n",
    "\n",
    "        # your code here\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Embedding layer that provides position information\n",
    "        # This allows the model to know where each token is in the sequence\n",
    "\n",
    "        # self.pos_embed = # TODO\n",
    "\n",
    "        # your code here\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Create a stack of transformer blocks\n",
    "        # Each block contains self-attention and feed-forward networks\n",
    "\n",
    "        # Initialize an empty list to hold the transformer blocks\n",
    "        transformer_blocks = []\n",
    "\n",
    "        # TODO : Create each transformer block individually and add it to the list (based on number of layers)\n",
    "\n",
    "        # your code here\n",
    "        for _ in range(n_layers):\n",
    "            transformer_blocks.append(\n",
    "                TransformerBlock(embed_dim=embed_dim,\n",
    "                                 num_heads=num_heads,\n",
    "                                 block_size=block_size,\n",
    "                                 ffw_hidden_dim=ffw_hidden_dim,\n",
    "                                 dropout=dropout)\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "\n",
    "        # Create the sequential container with all blocks\n",
    "        self.blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)  # final layer norm\n",
    "\n",
    "        # Output projection that converts embeddings back to vocabulary logits\n",
    "        # For each position, produces a probability distribution over the vocabulary\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False) # maps to word logits\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # (1) token + positional embeddings\n",
    "        # Combine token embeddings and positional embeddings\n",
    "        # Convert input token IDs to embeddings\n",
    "\n",
    "        tok_emb = self.token_embed(idx) # shape: (B, T, embed_dim)\n",
    "\n",
    "        # Get position encodings for each position in the sequence\n",
    "        pos_emb = self.pos_embed(torch.arange(T, device=idx.device)) # shape: (T, embed_dim)\n",
    "\n",
    "        # Add token and positional embeddings (broadcasting pos_emb to match batch dimension)\n",
    "        x = tok_emb + pos_emb.unsqueeze(0) # shape: (B, T, embed_dim)\n",
    "\n",
    "       # (2) Pass the embeddings through the transformer blocks\n",
    "        x = self.blocks(x) # shape: (B, T, embed_dim)\n",
    "\n",
    "        # (3) Apply final normalization and project to vocabulary logits\n",
    "        x = self.ln_f(x) # shape: (B, T, embed_dim)\n",
    "        logits = self.head(x) # shape: (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        # Calculate loss if targets are provided (during training)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # Reshape logits and targets for cross entropy loss calculation\n",
    "            logits_2d = logits.view(B*T, C)\n",
    "            targets_2d = targets.view(B*T)\n",
    "\n",
    "            # Calculate cross entropy loss\n",
    "            loss = nn.CrossEntropyLoss()(logits_2d, targets_2d)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "      # Text generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            # If context is too long, keep only the last block_size tokens\n",
    "\n",
    "            # idx_cond = # TODO\n",
    "\n",
    "            # your code here\n",
    "            idx_cond = idx[:, -self.block_size:] if idx.size(1) > self.block_size else idx\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             raise NotImplementedError\n",
    "\n",
    "            # Forward pass to get next token probabilities\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # We only need predictions for the last token in the sequence\n",
    "            # focus on last time step\n",
    "            # logits = # TODO\n",
    "\n",
    "            # your code here\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             raise NotImplementedError\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            # Remember that dimensions need to be modified\n",
    "            # probs = # TODO\n",
    "\n",
    "            # your code here\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             raise NotImplementedError\n",
    "\n",
    "            # Sample from the probability distribution\n",
    "            # We will use a multinomial distribution to get the next index\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append the new token to the sequence\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "gv3LQlqttpy_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40fdacc6cb9d49ab7666b6325b7954fe",
     "grade": true,
     "grade_id": "cell-a6793d3211de9a6e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ae847e52-c8c2-4354-9b36-3a46eac6e8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GPT class implementation...\n",
      "✓ idx_cond implementation is correct\n",
      "✓ Last token logits extraction is correct\n",
      "✓ Softmax application is correct\n",
      "All tests passed! The GPT class is implemented correctly.\n"
     ]
    }
   ],
   "source": [
    "def test_gpt_implementation():\n",
    "    \"\"\"\n",
    "    Test the implementation of the GPT class.\n",
    "    This test verifies that students have correctly implemented the missing components.\n",
    "    \"\"\"\n",
    "    print(\"Testing GPT class implementation...\")\n",
    "\n",
    "    # Set up test parameters\n",
    "    vocab_size = 100\n",
    "    embed_dim = 32\n",
    "    block_size = 10\n",
    "    n_layers = 2\n",
    "    num_heads = 4\n",
    "    ffw_hidden_dim = 64\n",
    "\n",
    "    # Create a small test input\n",
    "    batch_size = 2\n",
    "    seq_len = 8\n",
    "    idx = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Test the GPT initialization\n",
    "    model = GPT(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        block_size=block_size,\n",
    "        n_layers=n_layers,\n",
    "        num_heads=num_heads,\n",
    "        ffw_hidden_dim=ffw_hidden_dim\n",
    "    )\n",
    "\n",
    "    # 1. Test token and position embeddings\n",
    "    assert hasattr(model, 'token_embed'), \"GPT class missing 'token_embed' attribute\"\n",
    "    assert hasattr(model, 'pos_embed'), \"GPT class missing 'pos_embed' attribute\"\n",
    "\n",
    "    assert isinstance(model.token_embed, nn.Embedding), \"'token_embed' should be an instance of nn.Embedding\"\n",
    "    assert isinstance(model.pos_embed, nn.Embedding), \"'pos_embed' should be an instance of nn.Embedding\"\n",
    "\n",
    "    assert model.token_embed.weight.shape == (vocab_size, embed_dim), f\"Token embedding weight should be shape ({vocab_size}, {embed_dim})\"\n",
    "    assert model.pos_embed.weight.shape == (block_size, embed_dim), f\"Position embedding weight should be shape ({block_size}, {embed_dim})\"\n",
    "\n",
    "    # 2. Test transformer blocks\n",
    "    assert hasattr(model, 'blocks'), \"GPT class missing 'blocks' attribute\"\n",
    "    assert isinstance(model.blocks, nn.Sequential), \"'blocks' should be an instance of nn.Sequential\"\n",
    "    assert len(model.blocks) == n_layers, f\"There should be {n_layers} transformer blocks\"\n",
    "\n",
    "    # Check that each block is a TransformerBlock\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        assert isinstance(block, TransformerBlock), f\"Block {i} should be a TransformerBlock\"\n",
    "\n",
    "    # 3. Test forward pass\n",
    "    try:\n",
    "        logits, loss = model(idx, targets)\n",
    "        assert logits.shape == (batch_size, seq_len, vocab_size), f\"Logits shape should be ({batch_size}, {seq_len}, {vocab_size})\"\n",
    "        assert isinstance(loss, torch.Tensor), \"Loss should be a scalar torch.Tensor\"\n",
    "    except Exception as e:\n",
    "        assert False, f\"Forward pass failed: {str(e)}\"\n",
    "\n",
    "    # 4. Test generation method components directly\n",
    "\n",
    "    # Test idx_cond truncation\n",
    "    try:\n",
    "        # Create a test method that only tests the idx_cond part\n",
    "        def test_idx_cond():\n",
    "            long_context = torch.randint(0, vocab_size, (1, block_size + 5))\n",
    "            model.block_size = block_size  # Ensure block_size is set\n",
    "\n",
    "            # Get the idx_cond by running just that part of the code\n",
    "            idx_cond = long_context[:, -block_size:]\n",
    "\n",
    "            # Verify it was truncated correctly\n",
    "            assert idx_cond.shape[1] == block_size, \"idx_cond should be truncated to block_size\"\n",
    "            assert torch.equal(idx_cond, long_context[:, -block_size:]), \"idx_cond should contain the last block_size tokens\"\n",
    "\n",
    "            print(\"✓ idx_cond implementation is correct\")\n",
    "\n",
    "        test_idx_cond()\n",
    "    except Exception as e:\n",
    "        assert False, f\"idx_cond implementation test failed: {str(e)}\"\n",
    "\n",
    "    # Test last token logits selection\n",
    "    try:\n",
    "        # Create dummy logits tensor\n",
    "        dummy_logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Extract last token logits\n",
    "        last_token_logits = dummy_logits[:, -1, :]\n",
    "\n",
    "        # Verify shape\n",
    "        assert last_token_logits.shape == (batch_size, vocab_size), \"Last token logits should have shape (batch_size, vocab_size)\"\n",
    "\n",
    "        # Verify content\n",
    "        assert torch.equal(last_token_logits, dummy_logits[:, -1, :]), \"Last token logits should be extracted correctly\"\n",
    "\n",
    "        print(\"✓ Last token logits extraction is correct\")\n",
    "    except Exception as e:\n",
    "        assert False, f\"Last token logits test failed: {str(e)}\"\n",
    "\n",
    "    # Test softmax application\n",
    "    try:\n",
    "        # Create dummy logits\n",
    "        dummy_logits = torch.randn(batch_size, vocab_size)\n",
    "\n",
    "        # Apply softmax\n",
    "        probs = torch.softmax(dummy_logits, dim=-1)\n",
    "\n",
    "        # Verify shape\n",
    "        assert probs.shape == (batch_size, vocab_size), \"Probability distribution should have same shape as input logits\"\n",
    "\n",
    "        # Verify it's a valid probability distribution\n",
    "        assert torch.allclose(probs.sum(dim=-1), torch.ones(batch_size)), \"Probabilities should sum to 1 along vocabulary dimension\"\n",
    "        assert (probs >= 0).all() and (probs <= 1).all(), \"Probabilities should be between 0 and 1\"\n",
    "\n",
    "        print(\"✓ Softmax application is correct\")\n",
    "    except Exception as e:\n",
    "        assert False, f\"Softmax test failed: {str(e)}\"\n",
    "\n",
    "    print(\"All tests passed! The GPT class is implemented correctly.\")\n",
    "\n",
    "test_gpt_implementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b643fbd9b6eb733e582c7a5a4505619",
     "grade": false,
     "grade_id": "cell-c3474c8ece7e039c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The training may take too long if you leave the 'max_steps' at 1000. You might want to consider reducing the number of steps for faster training. You can also alter some of the other hyperparameters to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "YocXPxXtsE7z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d2ee3985cb22f75b13afabd34703edc",
     "grade": false,
     "grade_id": "cell-87c958f758bb9884",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "4928ffbc-01d1-41a0-f9a3-fb70a4075d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Train loss: 10.3409, Val loss: 10.3412\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 128\n",
    "n_layers = 4\n",
    "num_heads = 4\n",
    "ffw_hidden_dim = 256\n",
    "dropout = 0.1\n",
    "learning_rate = 3e-4\n",
    "max_steps = 1000\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    block_size=block_size,\n",
    "    n_layers=n_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffw_hidden_dim=ffw_hidden_dim,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # Get 'x' and 'y' batches for training using the get_batch function\n",
    "    # xb, yb = # TODO\n",
    "    # your code here\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n",
    "\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    # Compute the logits and the loss\n",
    "    # logits, loss = # TODO\n",
    "    # your code here\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        with torch.no_grad():\n",
    "            xb_val, yb_val = get_batch(split='val')\n",
    "            xb_val, yb_val = xb_val.to(device), yb_val.to(device)\n",
    "            _, val_loss = model(xb_val, yb_val)\n",
    "        print(f\"Step {step}, Train loss: {loss.item():.4f}, Val loss: {val_loss.item():.4f}\")\n",
    "        losses.append((loss.item(), val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "deletable": false,
    "editable": false,
    "id": "PRP9v0Jutxa_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec813046224cf94f26ab8a7993d15c51",
     "grade": false,
     "grade_id": "cell-9afe2a885178337a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1cc275d9-9638-4a03-9885-d00f1811bfd2"
   },
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([l[0] for l in losses], label='Train')\n",
    "plt.plot([l[1] for l in losses], label='Validation')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "KR2yWZbnsPAF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da37c2661e5e88b229e92459d7e26972",
     "grade": false,
     "grade_id": "cell-64e8b4102ef223dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e7c9ca91-1ab1-47d8-f3c7-2fcc5944168e"
   },
   "outputs": [],
   "source": [
    "# Example prompt\n",
    "initial_words = [\"House\"]\n",
    "context_ids = torch.tensor([ [word_to_idx[w] for w in initial_words] ], dtype=torch.long, device=device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(context_ids, max_new_tokens=20)\n",
    "\n",
    "generated_words = [idx_to_word[i.item()] for i in generated_ids[0]]\n",
    "generated_text = \" \".join(generated_words)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZnAfxt7R16Q3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2c995a6f491ef6d9fb37933b605bf53",
     "grade": false,
     "grade_id": "cell-70199268ee4d63fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This assignment provides a complete implementation of a GPT-style transformer language model trained on word-level data from Shakespeare's texts. Through examining each component—from attention heads to transformer blocks to the complete GPT architecture—we can understand how these models work from the ground up.\n",
    "\n",
    "\n",
    "Key takeaways from this implementation:\n",
    "\n",
    "- Attention Mechanism: The self-attention mechanism allows the model to focus on different parts of the input when making predictions, capturing relationships between words regardless of their distance in the text.\n",
    "- Transformer Architecture: The combination of multi-head attention, feed-forward networks, residual connections, and layer normalization creates a powerful and trainable architecture for sequence modeling.\n",
    "- Autoregressive Learning: The model is trained to predict the next word given previous words, which enables it to generate coherent text by repeatedly sampling from its learned distribution.\n",
    "- Positional Information: Since transformers process all tokens in parallel, positional embeddings are essential to maintain sequence order information.\n",
    "- Batched Training: The get_batch function efficiently samples training data, enabling effective learning from large text corpora.\n",
    "\n",
    "Despite the relative simplicity of this implementation (compared to state-of-the-art models with billions of parameters), it demonstrates the core principles that underlie modern language models like GPT-4o, Gemini, and Claude. Even with limited training and a small model size, the implementation is able to learn patterns from Shakespeare's language and generate text with Shakespearean characteristics.\n",
    "\n",
    "This provides valuable insights into how large language models work and serves as a foundation for understanding more complex implementations and innovations in the field of natural language processing.\n",
    "\n",
    "There are some over-simplifications in this assignment that you might want to correct. For example, words are usually not used as tokens. Sub-word tokens obtained through strategies like BPE are generally a better unit. Try implementing the same language model with subword tokens. You can also alter the number of heads, the size of the dataset, or any other hyperparameter.\n",
    "\n",
    "We have also not handled unknown tokens in our dataset. How would we change our code to handle this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
