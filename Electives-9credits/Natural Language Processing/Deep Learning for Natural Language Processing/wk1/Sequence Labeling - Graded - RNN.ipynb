{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eZkpWz16rKyz",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "237efc6f096f1a63c5ab29518bdd8e38",
     "grade": false,
     "grade_id": "cell-1c4fcb1f28b53847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Sequence Labeling with RNNs\n",
    "\n",
    "This lab introduces Recurrent Neural Networks (RNNs) and their application to sequence labeling tasks,\n",
    "specifically Part-of-Speech (POS) tagging using the Brown corpus.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "1. Recurrent Neural Networks (RNNs):\n",
    "   - Neural networks designed to work with sequential data\n",
    "   - Process input sequences one element at a time while maintaining a hidden state\n",
    "   - Can handle variable-length sequences\n",
    "   - Well-suited for tasks like language processing, time series analysis, and sequence labeling\n",
    "\n",
    "2. Long Short-Term Memory (LSTM):\n",
    "   - A sophisticated type of RNN that addresses the vanishing gradient problem\n",
    "   - Contains specialized gates (input, forget, output) to control information flow\n",
    "   - Better at capturing long-term dependencies in sequences\n",
    "   - More stable training compared to vanilla RNNs\n",
    "\n",
    "3. Sequence Labeling:\n",
    "   - Task of assigning a categorical label to each element in a sequence\n",
    "   - Examples include POS tagging, named entity recognition, and chunking\n",
    "   - Each input token maps to exactly one output label\n",
    "\n",
    "4. Part-of-Speech (POS) Tagging:\n",
    "   - Linguistic task of marking words with their grammatical categories\n",
    "   - Examples: noun, verb, adjective, determiner, etc.\n",
    "   - Context-dependent: same word can have different POS tags based on usage\n",
    "   - Essential for many downstream NLP tasks\n",
    "\n",
    "5. Key Components in Implementation:\n",
    "   - Word Embeddings: Dense vector representations of words\n",
    "   - LSTM Layer: Processes the sequence and captures contextual information\n",
    "   - Output Layer: Maps LSTM outputs to tag probabilities\n",
    "   - Cross-Entropy Loss: Standard loss function for classification tasks\n",
    "\n",
    "In this lab, we'll implement a complete sequence labeling system using PyTorch,\n",
    "demonstrating how to:\n",
    "- Process and prepare sequential data\n",
    "- Build an RNN-based model architecture\n",
    "- Train the model using backpropagation through time\n",
    "- Evaluate the model's performance on a held-out test set\n",
    "\n",
    "The Brown corpus serves as our dataset, providing pre-tagged sentences that we'll use\n",
    "for training and testing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "svi99oz3oJIc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d5cf404f4636889935b87856ba8d6cb",
     "grade": false,
     "grade_id": "cell-d350976cac050403",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Install and Import Required Libraries\n",
    "In some environments (such as Google Colab), you may need to install nltk first. If you are running locally and have nltk already, you can omit the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "X1h5Tb4FoQJI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "156e9f411a6516b8e846caea555f66d6",
     "grade": false,
     "grade_id": "cell-c29e653d478fbbb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c2179b14-ae4c-42f4-ffda-ba94e0910fb9"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "PeBLYWqvEg7O",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b547c67a22aee87e0cec25c7570ce360",
     "grade": false,
     "grade_id": "cell-4bacaf9eac3211b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "20d64b95-88a3-469a-9034-9d5f0e9e3248"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN-based Sequence Labeling (POS Tagging with Brown Corpus)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Utility function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Utility function to print all model parameters and their gradients\n",
    "def print_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        print(param.data)\n",
    "        print(param.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pdA7cIUsoXE2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d4ed875d8fdb492f5b6b54f855e606c",
     "grade": false,
     "grade_id": "cell-2e5fff28fb75a13b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Preparation\n",
    "Here, we are going to build a toy dataset from the Brown corpus for a simplified part-of-speech tagging task.\n",
    "\n",
    "- Load the Brown corpus: We'll retrieve sentences, each of which is a list of (word, tag) pairs.\n",
    "- Filter/Map Tags: Brown offers quite a few tags, but to keep it simpler, we’ll use the universal tagset which has 12 coarse-grained POS categories.\n",
    "- Split into Train and Test: We'll create a small train/test split so that we can train on one portion and test on another.\n",
    "- Convert to Indices: We need to build a vocabulary for words and a label set for tags. We'll map each unique word to an integer index and each unique tag to a label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "df-25AAYEg7S",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf0a3831dc15570cfa886593f5b562e",
     "grade": false,
     "grade_id": "cell-01c8785d90f5a591",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "24fe4ea0-e37c-422a-c5b7-3cc6527a7cb0"
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# 2.1. Load and Inspect Brown Corpus Sentences\n",
    "###############################################\n",
    "nltk_sentences = brown.tagged_sents(tagset='universal')  # Each sentence is list of (word, tag) pairs\n",
    "print(\"Total sentences in Brown (Universal Tagset):\", len(nltk_sentences))\n",
    "print(\"Example:\", nltk_sentences[0])\n",
    "\n",
    "#######################################\n",
    "# 2.2. Create a Small Subset of Data\n",
    "#######################################\n",
    "# For demonstration, we'll use fewer sentences.\n",
    "# You can expand this if you want more training data.\n",
    "n_samples = 200  # Feel free to increase\n",
    "sentences = nltk_sentences[:n_samples]\n",
    "\n",
    "###########################################\n",
    "# 2.3. Build Word and Tag Vocabulary\n",
    "###########################################\n",
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "word_counter = 0\n",
    "tag_counter = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    for (word, tag) in sent:\n",
    "        w = word.lower()  # convert to lowercase to reduce vocab size\n",
    "        if w not in word_to_idx:\n",
    "            word_to_idx[w] = word_counter\n",
    "            word_counter += 1\n",
    "        if tag not in tag_to_idx:\n",
    "            tag_to_idx[tag] = tag_counter\n",
    "            tag_counter += 1\n",
    "\n",
    "print(\"Size of word vocabulary:\", len(word_to_idx))\n",
    "print(\"Size of tag set:\", len(tag_to_idx))\n",
    "\n",
    "##########################################\n",
    "# 2.4. Convert Sentences into Tensor Form\n",
    "##########################################\n",
    "dataset = []\n",
    "for sent in sentences:\n",
    "    word_indices = []\n",
    "    tag_indices = []\n",
    "    for (word, tag) in sent:\n",
    "        word_indices.append(word_to_idx[word.lower()])\n",
    "        tag_indices.append(tag_to_idx[tag])\n",
    "    dataset.append( (torch.tensor(word_indices, dtype=torch.long),\n",
    "                     torch.tensor(tag_indices, dtype=torch.long)) )\n",
    "\n",
    "#######################################\n",
    "# 2.5. Split into Train and Test\n",
    "#######################################\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(dataset)*train_ratio)\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "print(\"Training samples:\", len(train_data))\n",
    "print(\"Test samples:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BBpjsCs-oqE2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9cca680242b0146e373fdff9166856f",
     "grade": false,
     "grade_id": "cell-af8930764bbef10c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Define the RNN Model for Sequence Labeling\n",
    "Our model will have:\n",
    "\n",
    "- An Embedding layer: Converts word indices to dense embeddings.\n",
    "- An LSTM: A unidirectional LSTM to process the sequence of embeddings.\n",
    "- An Output Linear layer: Projects the hidden states to the number of possible tags.\n",
    "- A Non-linear Activation (e.g., ReLU) if needed (often used inside or after LSTM outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "RGmZOvN4Eg7T",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a11fb9a62bf2b4e6147b61b8b615ee6",
     "grade": false,
     "grade_id": "cell-5d6430440774783b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "0f3ad379-683c-4045-f68e-063e52cc9fc2"
   },
   "outputs": [],
   "source": [
    "class SeqLabRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3.1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            # TODO: fill in with vocab_size, embedding_dim\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "        )\n",
    "\n",
    "        # 3.2. LSTM Layer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        # (input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, bias=False, batch_first=False, etc.)\n",
    "        self.lstm = nn.LSTM(\n",
    "            # TODO: embedding_dim,\n",
    "            # TODO: hidden_dim,\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "            num_layers=1,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # 3.3. Output Projection Layer\n",
    "        self.output_layer = nn.Linear(\n",
    "            # TODO: fill in with hidden_dim, output_dim\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "        )\n",
    "\n",
    "        # 3.4. (Optionally) a Non-linear activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq shape: [seq_len] (1D containing indices of words)\n",
    "\n",
    "        # a) Embedding\n",
    "        embeddings = self.embedding(input_seq)\n",
    "        # embeddings shape: [seq_len, embedding_dim]\n",
    "\n",
    "        # b) LSTM\n",
    "        # TODO: call LSTM on embeddings (note LSTM expects [seq_len, batch_size, embed_dim])\n",
    "        # lstm_output, (h_n, c_n) =  \n",
    "        \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # c) (Optionally) apply ReLU to LSTM output\n",
    "        #    But typically we apply the Linear layer directly on the LSTM outputs.\n",
    "        \n",
    "        # d) Output layer: transform LSTM outputs to tag predictions\n",
    "        output = self.output_layer(lstm_output)\n",
    "        # output shape: [seq_len, output_dim]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VdbN8AN0fiu-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c05fc5a0331aaed7e0e0f138de933c33",
     "grade": true,
     "grade_id": "cell-536704f1bae3d987",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_seqlabrnn_init():\n",
    "    # Initialize model with test parameters\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 64\n",
    "    hidden_dim = 128\n",
    "    output_dim = 10\n",
    "\n",
    "    model = SeqLabRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    # Test embedding layer dimensions\n",
    "    assert model.embedding.num_embeddings == vocab_size, \"Embedding vocabulary size incorrect\"\n",
    "    assert model.embedding.embedding_dim == embedding_dim, \"Embedding dimension incorrect\"\n",
    "\n",
    "    # Test LSTM layer dimensions\n",
    "    assert model.lstm.input_size == embedding_dim, \"LSTM input size incorrect\"\n",
    "    assert model.lstm.hidden_size == hidden_dim, \"LSTM hidden size incorrect\"\n",
    "    assert model.lstm.num_layers == 1, \"LSTM number of layers incorrect\"\n",
    "    assert model.lstm.bias == False, \"LSTM bias setting incorrect\"\n",
    "\n",
    "    # Test output layer dimensions\n",
    "    assert model.output_layer.in_features == hidden_dim, \"Output layer input size incorrect\"\n",
    "    assert model.output_layer.out_features == output_dim, \"Output layer output size incorrect\"\n",
    "\n",
    "    # Test forward pass with sample input\n",
    "    seq_len = 5\n",
    "    input_seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "    output = model(input_seq)\n",
    "    assert output.shape == (seq_len, output_dim), f\"Expected output shape {(seq_len, output_dim)}, got {output.shape}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_seqlabrnn_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f48a544e968fed401daa1538c29a9f2a",
     "grade": true,
     "grade_id": "cell-6319791965fce59d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_seqlabrnn_forward():\n",
    "    \"\"\"Test the forward method of SeqLabRNN\"\"\"\n",
    "    vocab_size, embedding_dim, hidden_dim, output_dim = 100, 50, 64, 10\n",
    "    model = SeqLabRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    # Create a sample input sequence\n",
    "    seq_len = 8\n",
    "    input_seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "\n",
    "    # Run forward pass\n",
    "    output = model(input_seq)\n",
    "\n",
    "    # Check output shape - should be [seq_len, output_dim]\n",
    "    assert output.shape == (seq_len, output_dim)\n",
    "    print(\"Forward pass test passed!\")\n",
    "\n",
    "test_seqlabrnn_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ld4FYTa4o5D-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e05ccfd61aba3899da62c650e945ef2",
     "grade": false,
     "grade_id": "cell-7b3d9d64284696ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Initialize the Model\n",
    "Below we define:\n",
    "\n",
    "vocab_size = total number of unique words.\n",
    "embedding_dim = dimension of word vectors.\n",
    "hidden_dim = dimension of LSTM hidden layer.\n",
    "output_dim = number of tags (unique labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3DY3UmLtEg7T",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba37f56159c328e01e8e7035e47482fb",
     "grade": false,
     "grade_id": "cell-352895f4a32a61d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 50   # can experiment\n",
    "hidden_dim = 64      # can experiment\n",
    "output_dim = len(tag_to_idx)\n",
    "\n",
    "model = SeqLabRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "print(\"Number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ELoI7I3OpDwF",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfd46d13a5b1b16d4398baa64d7e4bef",
     "grade": false,
     "grade_id": "cell-4f2c0333e9223f18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training Setup\n",
    "### Define Loss Function\n",
    "For multi-class sequence labeling, a common choice is the cross-entropy loss (nn.CrossEntropyLoss). Note that nn.CrossEntropyLoss in PyTorch expects raw logits of shape [N, C] (not softmax-ed) where:\n",
    "\n",
    "N is the total number of tokens (flattened across sequences),\n",
    "C is the number of classes.\n",
    "### Define Optimizer\n",
    "We can use SGD or Adam. Here, we’ll just use simple SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "S9alqs6DEg7T",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d92f44e0f195e215f9ea9863e5f52680",
     "grade": false,
     "grade_id": "cell-450c6a8671521c82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "# criterion = # TODO\n",
    "# optimizer = # TODO: add the optimizer with model.parameters(), experiment with different learning rates\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4MEji--xpeT7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "482641ec7b9e4e42fbf193c9d22fda30",
     "grade": true,
     "grade_id": "cell-f8c04687208a146b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_loss_optim():\n",
    "    # Test the loss function and optimizer\n",
    "    assert isinstance(criterion, nn.CrossEntropyLoss), \"Loss function should be CrossEntropyLoss\"\n",
    "\n",
    "    assert isinstance(optimizer, optim.SGD), \"Optimizer should be SGD\"\n",
    "    assert optimizer.defaults['lr'] == 0.1, \"Learning rate should be 0.1\"\n",
    "\n",
    "    # Check optimizer is connected to model parameters\n",
    "    model_params = list(model.parameters())\n",
    "    optimizer_params = list(optimizer.param_groups[0]['params'])\n",
    "    assert len(model_params) == len(optimizer_params), \"Optimizer should contain all model parameters\"\n",
    "\n",
    "    for mp, op in zip(model_params, optimizer_params):\n",
    "        assert mp is op, \"Optimizer parameter references should match model parameters\"\n",
    "\n",
    "    print(\"Loss function and optimizer test passed!\")\n",
    "\n",
    "test_loss_optim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QiVVTUNmpPrz",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9aa41c84a2b78e9e53a953233369e522",
     "grade": false,
     "grade_id": "cell-d522acf43c9eca96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training Loop\n",
    "We will train for a fixed number of epochs. During each epoch:\n",
    "\n",
    "- Set model to training mode: model.train()\n",
    "- Zero the gradients: optimizer.zero_grad()\n",
    "- Forward pass: Get the raw logits from the model for each token in the sentence.\n",
    "- Reshape your outputs/labels if needed so that they match [N, C] and [N].\n",
    "- Compute loss: Typically CrossEntropyLoss between your predictions and labels.\n",
    "- Backprop: loss.backward()\n",
    "- Update parameters: optimizer.step()\n",
    "In a real project, you might want to create mini-batches of sentences. Here, we’ll do a simple loop over each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "4u2oHOfNokLn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87258362d15c6ab638bc768e6b425fe5",
     "grade": false,
     "grade_id": "cell-7ed2c71a3b61e136",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5  # feel free to increase\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"### Epoch {epoch+1} ###\")\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in train_data:\n",
    "        # a) Zero out gradients from previous step\n",
    "        # TODO: optimizer.zero_grad()\n",
    "        \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # b) Forward pass: get raw scores for each token\n",
    "        # TODO: model(x)\n",
    "        \n",
    "        # y_raw = \n",
    "    \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # y_raw shape = [seq_len, output_dim]\n",
    "        # y shape = [seq_len]\n",
    "\n",
    "        # c) (Optionally) flatten so CrossEntropyLoss can handle shape = [N, C]\n",
    "        # Also ensure y is shape [N]\n",
    "        # For example:\n",
    "        # y_raw = y_raw.view(-1, output_dim)\n",
    "        # y = y.view(-1)\n",
    "\n",
    "        # d) Compute loss\n",
    "        # TODO: criterion(...)\n",
    "        # loss = \n",
    "        \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # e) Backprop\n",
    "        # TODO: loss.backward()\n",
    "        \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "    \n",
    "        # f) Update parameters\n",
    "        # TODO: optimizer.step()\n",
    "        \n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    print(\"Average train loss:\", avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NI4C3hcjpgGY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9de933e4c7885ebf1e207cc14060cfa2",
     "grade": true,
     "grade_id": "cell-d8de9d8fe3b0bd97",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_training_loop():\n",
    "    \"\"\"Test a simple training loop implementation\"\"\"\n",
    "    vocab_size, embedding_dim, hidden_dim, output_dim = 100, 50, 64, 10\n",
    "    model = SeqLabRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "    # Setup optimizer and loss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create a mini training set\n",
    "    mini_train_data = []\n",
    "    for _ in range(3):\n",
    "        seq_len = 5\n",
    "        x = torch.randint(0, vocab_size, (seq_len,))\n",
    "        y = torch.randint(0, output_dim, (seq_len,))\n",
    "        mini_train_data.append((x, y))\n",
    "\n",
    "    # Run a single training step on each sample\n",
    "    initial_loss = 0\n",
    "    model.train()\n",
    "    for x, y in mini_train_data:\n",
    "        optimizer.zero_grad()\n",
    "        y_raw = model(x)\n",
    "        loss = criterion(y_raw.view(-1, output_dim), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        initial_loss += loss.item()\n",
    "\n",
    "    # Run another epoch to see if loss decreases\n",
    "    final_loss = 0\n",
    "    for x, y in mini_train_data:\n",
    "        optimizer.zero_grad()\n",
    "        y_raw = model(x)\n",
    "        loss = criterion(y_raw.view(-1, output_dim), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    print(f\"Initial average loss: {initial_loss/len(mini_train_data)}\")\n",
    "    print(f\"Final average loss: {final_loss/len(mini_train_data)}\")\n",
    "    print(\"Training loop test passed!\")\n",
    "\n",
    "test_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tJFLcxmIpdi8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e30948fb2509f65ac812b9bc77ef577c",
     "grade": false,
     "grade_id": "cell-653aa3c115c2c4a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Testing/Evaluation\n",
    "Typically, you would:\n",
    "\n",
    "- Set model to eval mode (model.eval()).\n",
    "- Turn off gradient computations with torch.no_grad().\n",
    "- Loop over your test_data and accumulate predictions.\n",
    "- Compute accuracy or other metrics (e.g. F1 for certain tagging tasks).\n",
    "Below is a simple snippet to see how one might evaluate token-level accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Z3F948vrphXs",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "577705b1dbe48e40068a15e83b8ac397",
     "grade": false,
     "grade_id": "cell-b27eae3c85c480f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, y) in test_data:\n",
    "        logits = model(x)\n",
    "        # shape: [seq_len, output_dim]\n",
    "\n",
    "        # We pick the highest scoring tag\n",
    "        predicted_tags = logits.argmax(dim=-1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += (predicted_tags == y).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
