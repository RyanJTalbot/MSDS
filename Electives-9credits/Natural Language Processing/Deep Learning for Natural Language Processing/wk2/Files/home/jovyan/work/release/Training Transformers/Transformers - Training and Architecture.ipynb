{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "joBpVSx4V70z",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92e12e46dce0fce5ba67b804bfd74f6c",
     "grade": false,
     "grade_id": "cell-8d8f5d59ce8f42d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Transformers\n",
    "\n",
    "In this assignment, you will build and train a neural network for text classification using PyTorch. You will start by preprocessing text data, constructing a vocabulary, and encoding sentences into numerical representations. Using this processed data, you will implement key components of a Transformer-based model, including positional encoding, multi-head attention, and a feedforward network.\n",
    "\n",
    "Through this assignment, you will:\n",
    "\n",
    "- Develop a text preprocessing pipeline, including tokenization and vocabulary construction.\n",
    "- Implement custom dataset classes to handle text data efficiently in PyTorch.\n",
    "- Gain a deeper understanding of self-attention mechanisms and their role in modern NLP architectures.\n",
    "- Train and evaluate a Transformer-like model for spam classification.\n",
    "- Explore the impact of key hyperparameters on model performance.\n",
    "- By the end, you will have hands-on experience with text-based deep learning, helping you understand how Transformers process and classify textual inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SzDsSi7dHXoS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fee2fcc47fe07ca99dc69d7594e3abf",
     "grade": false,
     "grade_id": "cell-189293a0aac4f7ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RwKMQqFHQA4_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c56bbb951eb8915ee0c68b3c80e2aee",
     "grade": false,
     "grade_id": "cell-701472e25ce7e032",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The text preprocessor\n",
    "\n",
    "The TextPreprocessor class is a fundamental component for natural language processing tasks, designed to transform raw text into a format suitable for machine learning models. This preprocessor handles several critical text processing steps including tokenization, vocabulary building, and encoding sequences to fixed-length numeric representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "id": "qVhGTi8SHbRE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69fa36ba4996dc344e88c509f99f3014",
     "grade": false,
     "grade_id": "cell-84d014d965fe9ca4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_vocab_size=10000, max_seq_length=128):\n",
    "        # Initialize maximum vocabulary size (default 10000 words)\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        # Set maximum sequence length for padded outputs (default 128 tokens)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # Initialize word-to-index mapping with special tokens PAD (0) and UNK (1)\n",
    "        #self.word2idx = #TODO\n",
    "        # your code here\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Initialize index-to-word mapping with special tokens\n",
    "        #self.idx2word = #TODO\n",
    "        # your code here\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Initialize Counter object to track word frequencies\n",
    "        self.word_freq = Counter()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Convert input text to lowercase for consistency\n",
    "        #text = #TODO\n",
    "        # your code here\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Remove special characters while preserving basic punctuation\n",
    "        # text = #TODO\n",
    "        # your code here\n",
    "        text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Split text into individual tokens\n",
    "        return text.split()\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # Iterate through all texts and count word frequencies\n",
    "        for text in texts:\n",
    "            self.word_freq.update(self.tokenize(text))\n",
    "\n",
    "        # Sort words by frequency and keep top max_vocab_size-2 words\n",
    "        # vocab_words = #TODO\n",
    "        # your code here\n",
    "        vocab_words = self.word_freq.most_common()\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        vocab_words = vocab_words[:self.max_vocab_size-2]  # Reserve space for PAD and UNK\n",
    "\n",
    "        # Create mappings between words and indices for the vocabulary\n",
    "        for word, _ in vocab_words:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert text to tokens\n",
    "        # tokens = #TODO\n",
    "        # your code here\n",
    "        \n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Truncate sequence if longer than max_seq_length\n",
    "        # tokens = #TODO\n",
    "        # your code here\n",
    "        \n",
    "        tokens = tokens[:self.max_seq_length]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Pad sequence with PAD tokens if shorter than max_seq_length\n",
    "        # tokens = #TODO\n",
    "        # your code here\n",
    "        if len(tokens) < self.max_seq_length:\n",
    "            tokens = tokens + ['<PAD>'] * (self.max_seq_length - len(tokens))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Convert tokens to indices, using UNK for unknown words\n",
    "        return [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5da814fa3001792c4551d0e7ffb566a",
     "grade": true,
     "grade_id": "cell-35a6e8ac9df22a09",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5b6be0505541be185b72c83710dd4d5",
     "grade": true,
     "grade_id": "cell-22cd1ae33ff32c99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for text tokenization\n",
    "processor = TextPreprocessor()\n",
    "text = \"Hello, World! This is a TEST.\"\n",
    "tokens = processor.tokenize(text)\n",
    "assert all(t.islower() for t in tokens), \"Tokens should be lowercase\"\n",
    "assert ',' not in tokens and '!' not in tokens, \"Special characters should be removed\"\n",
    "assert \"hello\" in tokens and \"is\" in tokens and \"test\" in tokens, \"Words should be properly tokenized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "291e7ee20f429d96d135022ce5f0d521",
     "grade": true,
     "grade_id": "cell-8cd2dbd0f0fbcaa2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for vocabulary building\n",
    "processor = TextPreprocessor(max_vocab_size=5)  # PAD, UNK + 3 words\n",
    "sample_texts = [\"hello world\", \"hello test\", \"world test test\"]\n",
    "processor.build_vocab(sample_texts)\n",
    "assert len(processor.word2idx) == 5, \"Vocabulary size should respect max_vocab_size\"\n",
    "assert '<PAD>' in processor.word2idx and '<UNK>' in processor.word2idx, \"Special tokens should be in vocabulary\"\n",
    "assert 'test' in processor.word2idx, \"'test' should be in vocabulary (most frequent)\"\n",
    "assert 'hello' in processor.word2idx, \"'hello' should be in vocabulary\"\n",
    "assert 'world' in processor.word2idx, \"'world' should be in vocabulary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a7fa85f48feae6db83908e68041670",
     "grade": true,
     "grade_id": "cell-b57ce9760abce19f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for text encoding\n",
    "processor = TextPreprocessor(max_seq_length=5)\n",
    "processor.word2idx = {'<PAD>': 0, '<UNK>': 1, 'hello': 2, 'world': 3}\n",
    "processor.idx2word = {0: '<PAD>', 1: '<UNK>', 2: 'hello', 3: 'world'}\n",
    "\n",
    "# Test case for truncation\n",
    "encoded = processor.encode(\"hello world hello world hello world\")\n",
    "assert len(encoded) == 5, \"Sequence should be truncated to max_seq_length\"\n",
    "assert encoded == [2, 3, 2, 3, 2], \"Sequence should be properly encoded\"\n",
    "\n",
    "# Test case for padding\n",
    "encoded = processor.encode(\"hello world\")\n",
    "assert len(encoded) == 5, \"Sequence should be padded to max_seq_length\"\n",
    "assert encoded == [2, 3, 0, 0, 0], \"Sequence should be padded with PAD tokens\"\n",
    "\n",
    "# Test case for unknown tokens\n",
    "encoded = processor.encode(\"hello test\")\n",
    "assert encoded == [2, 1, 0, 0, 0], \"Unknown words should be encoded as UNK token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xOHb3PpyP3A6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3ab05b600e833a62d96a1da9513dbb1",
     "grade": false,
     "grade_id": "cell-8a2e2fe85b50fb84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The class is particularly important because it handles several crucial preprocessing steps that are essential for most NLP tasks. The tokenization process breaks down text into meaningful units while removing unnecessary characters. The vocabulary building functionality ensures that the most frequent words are retained while managing vocabulary size constraints, which is crucial for model efficiency and memory management. The encoding process converts variable-length text sequences into fixed-length numeric sequences that can be processed by neural networks.\n",
    "\n",
    "What makes this class especially valuable is its handling of edge cases through special tokens: '<PAD>' for maintaining consistent sequence lengths and '<UNK>' for handling out-of-vocabulary words. These features make it robust and suitable for real-world applications where text input can be highly variable and unpredictable.\n",
    "\n",
    "The class supports customization through its parameters (max_vocab_size and max_seq_length), making it adaptable to different requirements and hardware constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Nwlx_sBXQtxC",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e8c1008494a8e78612d3c278b4173e7",
     "grade": false,
     "grade_id": "cell-341f1df03bdb2e4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now define a class for our ham/spam dataset. The class is provided to you directly - no changes are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AMz2P0V3Hm4C",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e65c874cf97aab34d8d04c7cfc970447",
     "grade": false,
     "grade_id": "cell-d376a211d498c501",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, preprocessor):\n",
    "        self.encoded_texts = [preprocessor.encode(text) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.encoded_texts[idx], dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IgrSYOOyQ18u",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "957b0e9e77cd529914cde55150737b55",
     "grade": false,
     "grade_id": "cell-93f92fdb7e478bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# What are positional encodings?\n",
    "\n",
    "The PositionalEncoding class is a crucial component in transformer-based architectures, designed to inject information about the relative or absolute position of tokens in a sequence. This positional information is essential because transformer models process input tokens in parallel, which means they have no inherent way to understand the order of the sequence. Without positional encoding, a transformer would treat \"The cat sat on the mat\" and \"The mat sat on the cat\" as equivalent sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "id": "e6VlmF0gGEff",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39f11e534ad56b405d0724a67efa1b27",
     "grade": false,
     "grade_id": "cell-fa7a915f470122a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of shape [max_seq_length, d_model]\n",
    "        #pe = #TODO\n",
    "        # your code here\n",
    "        pe = torch.zeros(max_seq_length, d_model, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Create position vector [max_seq_length, 1]\n",
    "        # position = #TODO\n",
    "        # your code here\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Create division term [d_model/2]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices\n",
    "        # pe[:, 0::2] = #TODO\n",
    "        # your code here\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        # pe[:, 1::2] = #TODO\n",
    "        # your code here\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].shape[1]])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Add batch dimension [1, max_seq_length, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as buffer (won't be trained)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, d_model]\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19c0021d35c37c879ce4e3b4f4a03bae",
     "grade": true,
     "grade_id": "cell-8e3d0210fe023fca",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for positional encoding\n",
    "import torch\n",
    "import math\n",
    "\n",
    "d_model = 64\n",
    "max_seq_length = 10\n",
    "pe = torch.zeros(max_seq_length, d_model)\n",
    "position = torch.arange(0, max_seq_length).unsqueeze(1).float()\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "# Test computation of positional encoding\n",
    "assert pe.shape == (max_seq_length, d_model), \"PE matrix should have shape [max_seq_length, d_model]\"\n",
    "assert position.shape == (max_seq_length, 1), \"Position vector should have shape [max_seq_length, 1]\"\n",
    "\n",
    "# Apply sine to even indices\n",
    "pe_test = pe.clone()\n",
    "pe_test[:, 0::2] = torch.sin(position * div_term)\n",
    "assert not torch.allclose(pe_test, torch.zeros_like(pe_test)), \"Sine values should be non-zero\"\n",
    "assert torch.all((pe_test[:, 0::2] >= -1) & (pe_test[:, 0::2] <= 1)), \"Sine values should be between -1 and 1\"\n",
    "\n",
    "# Apply cosine to odd indices\n",
    "pe_test = pe.clone()\n",
    "pe_test[:, 1::2] = torch.cos(position * div_term)\n",
    "assert not torch.allclose(pe_test, torch.zeros_like(pe_test)), \"Cosine values should be non-zero\"\n",
    "assert torch.all((pe_test[:, 1::2] >= -1) & (pe_test[:, 1::2] <= 1)), \"Cosine values should be between -1 and 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lBx4T6cAQUr3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c6f78c5dbabb72aa42182cc0a7f3e12",
     "grade": false,
     "grade_id": "cell-0d0c2e42e9257120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The positional encoding follows a specific pattern using sine and cosine functions of different frequencies. This creates a unique encoding for each position that the model can learn to interpret. The encoding is deterministic and has several important properties:\n",
    "\n",
    "1. It allows the model to learn relative positions between tokens at different scales\n",
    "2. It has a bounded range, which helps with training stability\n",
    "3. The pattern can theoretically extend to sequences longer than those seen during training\n",
    "4. It creates unique patterns that vary smoothly with position\n",
    "\n",
    "The forward method simply adds these position encodings to the input embeddings. This addition is possible because the encodings are designed to have the same dimensionality as the token embeddings. The use of both sine and cosine functions allows the model to attend to relative positions through linear combinations of these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ei1XkLzwQ6kS",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3019913673ad6e7771eec5247607ce57",
     "grade": false,
     "grade_id": "cell-ae96935957af4831",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Understanding multi-head attention\n",
    "\n",
    "The MultiHeadAttention class is a fundamental component of transformer architectures that enables the model to attend to different parts of the input sequence simultaneously. This mechanism allows the model to capture various types of relationships and patterns in the data at different representation subspaces. The multi-head approach splits the attention computation into several parallel heads, each focusing on different aspects of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "id": "lu2GRxPBGIle",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f32200f61e7626e5b59d93fd151837f6",
     "grade": false,
     "grade_id": "cell-5905630cfcdce65d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # How do you compute d_k?\n",
    "        #self.d_k = #TODO\n",
    "        # your code here\n",
    "        self.d_k = self.d_model // self.num_heads\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Q, K, V shapes: [batch_size, num_heads, seq_length, d_k]\n",
    "        # Calculate attention scores\n",
    "        # scores = #TODO\n",
    "        # your code here\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        # attention_weights = #TODO\n",
    "        # your code here\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Calculate output - remember that you need to multiply attention weights and values\n",
    "        # output = #TODO\n",
    "        # your code here\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Linear projections and reshape\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention using scaled dot product with updated Q, K, V values\n",
    "        # output, attention_weights = #TODO\n",
    "        # your code here\n",
    "        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Reshape and apply output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dddb25a0ad40281ac3fc1506369a0f0",
     "grade": true,
     "grade_id": "cell-ee90f65ad42888df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for d_k computation in MultiHeadAttention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "assert multi_head_attention.d_k == 8, \"d_k should be d_model / num_heads\"\n",
    "\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "assert multi_head_attention.d_k == 32, \"d_k should be d_model / num_heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cea84e65c94b0ff92110b4fafc5aaab3",
     "grade": true,
     "grade_id": "cell-4fd3671cabcccdc5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test scaled_dot_product_attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "\n",
    "Q = torch.randn(batch_size, num_heads, seq_length, d_k)\n",
    "K = torch.randn(batch_size, num_heads, seq_length, d_k)\n",
    "V = torch.randn(batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Test for scores calculation\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "assert scores.shape == (batch_size, num_heads, seq_length, seq_length), \"Attention scores shape is incorrect\"\n",
    "\n",
    "# Test for softmax application\n",
    "attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "assert attention_weights.shape == (batch_size, num_heads, seq_length, seq_length), \"Attention weights shape is incorrect\"\n",
    "assert torch.allclose(attention_weights.sum(dim=-1), torch.ones(batch_size, num_heads, seq_length)), \"Softmax weights should sum to 1\"\n",
    "\n",
    "# Test for output calculation\n",
    "output = torch.matmul(attention_weights, V)\n",
    "assert output.shape == (batch_size, num_heads, seq_length, d_k), \"Output shape is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ApM-kbwXRjdG",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8608c2e6e7d4cc1289c7a24920816955",
     "grade": false,
     "grade_id": "cell-3c92923b92440d8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Defining the transformer block\n",
    "\n",
    "The FeedForward class implements a position-wise feed-forward network that applies two linear transformations with a ReLU activation in between. This component is crucial because it introduces non-linearity into the model and allows it to process the attention outputs further. The network typically expands the input dimension (d_model) to a larger intermediate dimension (d_ff) and then projects it back, enabling it to capture more complex patterns in the data.\n",
    "The TransformerBlock class combines all the essential components of a transformer layer into a single module. It implements the core transformer architecture with several important features:\n",
    "\n",
    "1. Multi-head attention for capturing relationships between different positions in the sequence\n",
    "2. Position-wise feed-forward network for additional processing\n",
    "3. Layer normalization for stable training\n",
    "4. Residual connections to prevent degradation in deep networks\n",
    "5. Dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "qwX0UCK5GqH3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f7d4e4e3d4354349beac16256b68480",
     "grade": false,
     "grade_id": "cell-84c461c8fcf90eac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        # Inherit from PyTorch's Module class\n",
    "        super().__init__()\n",
    "        # First linear transformation that expands the dimension\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # Second linear transformation that projects back to original dimension\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first linear transform, ReLU activation, and second linear transform\n",
    "        # This creates a non-linear transformation of the input\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        # Inherit from PyTorch's Module class\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize multi-head attention mechanism\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        # Initialize position-wise feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "\n",
    "        # Layer normalization for attention output\n",
    "        # self.norm1 = #TODO\n",
    "        # your code here\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # Layer normalization for feed-forward output\n",
    "        # self.norm2 = #TODO\n",
    "        # your code here\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Dropout for regularization - add the dropout layer\n",
    "        # self.dropout = #TODO\n",
    "        # your code here\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply multi-head attention with residual connection and normalization\n",
    "        attention_output, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # Apply feed-forward network with residual connection and normalization\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4554f490268e0ff94711169372731181",
     "grade": true,
     "grade_id": "cell-7e49f8bd2bab1b5d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for MultiHeadAttention forward method\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "Q = torch.randn(batch_size, seq_length, d_model)\n",
    "K = torch.randn(batch_size, seq_length, d_model)\n",
    "V = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "output, attention_weights = multi_head_attention.scaled_dot_product_attention(\n",
    "    multi_head_attention.W_q(Q).view(batch_size, -1, num_heads, d_model // num_heads).transpose(1, 2),\n",
    "    multi_head_attention.W_k(K).view(batch_size, -1, num_heads, d_model // num_heads).transpose(1, 2),\n",
    "    multi_head_attention.W_v(V).view(batch_size, -1, num_heads, d_model // num_heads).transpose(1, 2)\n",
    ")\n",
    "\n",
    "assert output.shape == (batch_size, num_heads, seq_length, d_model // num_heads), \"Output shape is incorrect\"\n",
    "assert attention_weights.shape == (batch_size, num_heads, seq_length, seq_length), \"Attention weights shape is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QmgsFMoCSRdu",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf28a38fb646ac283867cbc51c0a98ae",
     "grade": false,
     "grade_id": "cell-e055a4c9b2cbde7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The forward pass of the TransformerBlock processes the input through these components in sequence, with each step building upon the previous one. The use of residual connections (implemented through addition operations) helps gradient flow during training and allows the model to maintain access to lower-level features. The layer normalization after each sub-layer helps stabilize the learning process by normalizing the activations, while dropout helps prevent overfitting by randomly zeroing out some connections during training.\n",
    "Together, these components create a powerful building block that can be stacked multiple times to create deep transformer networks capable of processing complex sequential data across various domains, from natural language processing to computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Fyvu8ITASwRd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62bdf614385a8f83551e91d7c13ae3c8",
     "grade": false,
     "grade_id": "cell-1fec74ad86fbe7ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Adapting this architecture to our classification task\n",
    "\n",
    "The SpamTransformer class implements a complete transformer-based model specifically designed for spam detection tasks. This implementation combines all the previously discussed components into a cohesive architecture that can learn to distinguish between spam (unwanted) and ham (legitimate) messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D4Uu8qN7GzHU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b47d8d9234d6995e0239a39ef99fbad",
     "grade": false,
     "grade_id": "cell-9e1d22b5b11aebff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SpamTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_length):\n",
    "        # Inherit from PyTorch's Module class\n",
    "        super().__init__()\n",
    "\n",
    "        # Create embedding layer to convert token IDs to dense vectors\n",
    "        # Maps each token in vocabulary to a d_model dimensional vector\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Add positional encoding to provide position information\n",
    "        # This helps the model understand token ordering\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Create stack of transformer blocks\n",
    "        # Each block contains multi-head attention and feed-forward network\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final classification layer\n",
    "        # Projects to 2 dimensions for binary classification (spam/ham)\n",
    "        self.final_layer = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Convert input tokens to embeddings and add positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass input through each transformer block sequentially\n",
    "        # Each block processes and refines the sequence representation\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Average pooling across sequence length\n",
    "        # This creates a fixed-size representation of the entire sequence\n",
    "        x = torch.mean(x, dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        # Projects to logits for spam/ham classification\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a23e16bd05f38a7a521bf38ca658cc9",
     "grade": true,
     "grade_id": "cell-55be68f104be92bd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for TransformerBlock layer normalization and dropout\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Test layer normalization initialization\n",
    "layer_norm = torch.nn.LayerNorm(d_model)\n",
    "assert layer_norm.normalized_shape == (d_model,), \"Layer norm should normalize over d_model dimension\"\n",
    "\n",
    "# Test dropout initialization\n",
    "dropout = torch.nn.Dropout(dropout_rate)\n",
    "assert dropout.p == dropout_rate, \"Dropout probability should match the input rate\"\n",
    "\n",
    "# Test in the context of a transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff, dropout_rate)\n",
    "assert isinstance(transformer_block.norm1, torch.nn.LayerNorm), \"norm1 should be a LayerNorm instance\"\n",
    "assert isinstance(transformer_block.norm2, torch.nn.LayerNorm), \"norm2 should be a LayerNorm instance\"\n",
    "assert isinstance(transformer_block.dropout, torch.nn.Dropout), \"dropout should be a Dropout instance\"\n",
    "assert transformer_block.dropout.p == dropout_rate, \"Dropout rate should be set correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kTyMNXYzTEtH",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a21f172488d5ddbee94922afdffc70ff",
     "grade": false,
     "grade_id": "cell-8614d2c77d1caa0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's understand what is going on with the transformer architecture here:\n",
    "- The embedding layer converts discrete tokens into continuous vectors, allowing the model to learn semantic relationships between words.\n",
    "- The positional encoding ensures the model can understand the sequence order of tokens, which is crucial for understanding message context.\n",
    "- The stack of transformer blocks enables the model to process the input at multiple levels of abstraction, capturing both local and global patterns in the text.\n",
    "- Global average pooling creates a fixed-size representation of the entire message, regardless of its length, which is then used for classification.\n",
    "The final linear layer produces logits for binary classification, which can be converted to probabilities using softmax.\n",
    "\n",
    "Hopefully, through this exercise you can see how transformers can:\n",
    "\n",
    "- Handle variable-length input sequences\n",
    "- Capture complex relationships between words and phrases\n",
    "- Learn patterns at different scales through their multi-layer structure\n",
    "- Maintain position sensitivity while processing tokens in parallel\n",
    "- Prevent overfitting by using dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5eVvffcTT8Q2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d5644a3f6c9c42d009392573f0e14e1",
     "grade": false,
     "grade_id": "cell-79b20e6774f3cdd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Training and evaluation\n",
    "\n",
    "These functions form the core training loop for the spam detection transformer. The create_attention_mask function generates a simple attention mask that allows all positions to attend to all other positions. We don't mask padded tokens here, but that could be an option in a more complex task or a production setting.\n",
    "\n",
    "The train_epoch() function handles training for one entire epoch. You should be able to figure out how the rest of this notebook works as you are familiar with training and evaluation at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "id": "ZvNBG9tiG2yw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2bd7a2a6d9daa140d11139aa0a984b9",
     "grade": false,
     "grade_id": "cell-7b053d8e34373f2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_attention_mask(seq_length):\n",
    "    # Create a square matrix of ones with dimensions [seq_length, seq_length]\n",
    "    # This mask allows each position to attend to all other positions\n",
    "    # In a more complex implementation, this could be modified to mask padding tokens\n",
    "    # mask = #TODO\n",
    "    # your code here\n",
    "    \n",
    "    mask = torch.ones(seq_length, seq_length, dtype=torch.bool)\n",
    "    \n",
    "    \n",
    "#     raise NotImplementedError\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    # Set model to training mode - enables dropout, batch normalization, etc.\n",
    "    model.train()\n",
    "\n",
    "    # Initialize tracking variables for loss and accuracy\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through batches in the dataloader\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # Move data and target tensors to specified device (CPU/GPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Create attention mask for the current batch\n",
    "        mask = create_attention_mask(data.size(1)).to(device)\n",
    "\n",
    "        # Clear gradients from previous batch\n",
    "        #TODO\n",
    "\n",
    "        # Forward pass: compute model predictions\n",
    "        #TODO\n",
    "\n",
    "        # Calculate loss between predictions and targets\n",
    "        # loss = #TODO\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        #TODO\n",
    "\n",
    "        # Update model parameters using optimizer\n",
    "        #TODO\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(data, mask)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Accumulate total loss for epoch\n",
    "        # total_loss += #TODO\n",
    "        # your code here\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "\n",
    "        # Calculate and accumulate accuracy statistics\n",
    "        # pred = #TODO  # Get predicted class using argmax\n",
    "        # your code here\n",
    "        \n",
    "        pred = outputs.argmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        correct += pred.eq(target).sum().item()  # Count correct predictions\n",
    "        total += target.size(0)  # Count total predictions\n",
    "\n",
    "        # Print progress every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "\n",
    "    # Return average loss and accuracy for the epoch\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32c774f6b29046bc11c99e7a53f82e4f",
     "grade": true,
     "grade_id": "cell-9d05489d1d7a31f4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for create_attention_mask function\n",
    "seq_length = 5\n",
    "mask = torch.ones(seq_length, seq_length)\n",
    "assert mask.shape == (seq_length, seq_length), \"Mask shape should be [seq_length, seq_length]\"\n",
    "assert torch.all(mask == 1), \"All values in the mask should be 1\"\n",
    "\n",
    "# Test for optimizer operations in train_epoch\n",
    "model = SpamTransformer(vocab_size=100, d_model=64, num_heads=4, d_ff=256, num_layers=2, max_seq_length=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Test for optimizer.zero_grad()\n",
    "initial_grads = [param.grad for param in model.parameters() if param.grad is not None]\n",
    "optimizer.zero_grad()\n",
    "after_zero_grads = [param.grad for param in model.parameters() if param.grad is not None]\n",
    "assert len(after_zero_grads) == 0, \"All gradients should be None after zero_grad()\"\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "data = torch.randint(0, 100, (batch_size, seq_length))\n",
    "target = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "# Test for forward pass\n",
    "output = model(data)\n",
    "assert output.shape == (batch_size, 2), \"Output shape should be [batch_size, num_classes]\"\n",
    "\n",
    "# Test for loss calculation\n",
    "loss = criterion(output, target)\n",
    "assert isinstance(loss, torch.Tensor), \"Loss should be a tensor\"\n",
    "assert loss.requires_grad, \"Loss should require gradients\"\n",
    "\n",
    "# Test for backward pass\n",
    "loss.backward()\n",
    "has_grad = [param.grad is not None for param in model.parameters()]\n",
    "assert all(has_grad), \"All parameters should have gradients after backward()\"\n",
    "\n",
    "# Test for optimizer step\n",
    "old_params = [param.clone().detach() for param in model.parameters()]\n",
    "optimizer.step()\n",
    "new_params = [param.clone().detach() for param in model.parameters()]\n",
    "assert any(not torch.allclose(old, new) for old, new in zip(old_params, new_params)), \"Parameters should change after optimizer step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "id": "clu1S3_8G6bj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d527ceaa21c3eedc468982fbafacbbd3",
     "grade": false,
     "grade_id": "cell-1d21f079f51ac3ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    # Set model to evaluation mode - disables dropout, batch normalization, etc.\n",
    "    model.eval()\n",
    "    # Initialize tracking variables\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            # Move data to appropriate device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Create attention mask for current batch\n",
    "            # mask = #TODO\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "\n",
    "            # Forward pass\n",
    "            # output = #TODO\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            # Calculate loss\n",
    "            # loss = #TODO\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            # Accumulate loss and accuracy metrics\n",
    "            total_loss += loss.item()\n",
    "            # pred = #TODO # use argmax\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(dataloader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94852e53f955b19125f314cdaf19b9fb",
     "grade": true,
     "grade_id": "cell-85b404c79638e25d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for evaluate function components\n",
    "model = SpamTransformer(vocab_size=100, d_model=64, num_heads=4, d_ff=256, num_layers=2, max_seq_length=10)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "data = torch.randint(0, 100, (batch_size, seq_length))\n",
    "target = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "# Test for attention mask creation\n",
    "mask = torch.ones(seq_length, seq_length)\n",
    "assert mask.shape == (seq_length, seq_length), \"Mask shape should be [seq_length, seq_length]\"\n",
    "\n",
    "# Test for model output\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "assert output.shape == (batch_size, 2), \"Output shape should be [batch_size, num_classes]\"\n",
    "\n",
    "# Test for loss calculation\n",
    "with torch.no_grad():\n",
    "    loss = criterion(output, target)\n",
    "assert isinstance(loss, torch.Tensor), \"Loss should be a tensor\"\n",
    "\n",
    "# Test for prediction\n",
    "with torch.no_grad():\n",
    "    pred = output.argmax(dim=1)\n",
    "assert pred.shape == (batch_size,), \"Prediction shape should be [batch_size]\"\n",
    "assert torch.all((pred == 0) | (pred == 1)), \"Predictions should be binary (0 or 1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hT-CdUcKN9mt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90c8eac783a00641a74b14aca29c351b",
     "grade": false,
     "grade_id": "cell-a4cc7ca584cb4089",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot training and validation losses over epochs.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses per epoch\n",
    "        val_losses (list): List of validation losses per epoch\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make sure the plot is displayed even in non-interactive mode\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 130583,
     "status": "ok",
     "timestamp": 1739198992089,
     "user": {
      "displayName": "Bhargav Shandilya",
      "userId": "07528554298385509222"
     },
     "user_tz": 420
    },
    "id": "q2pPl05VG-Qm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14dfacc5fabe2537b4da6ae66fe4f797",
     "grade": false,
     "grade_id": "cell-84337df65a525441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d3991d1a-c56b-4983-c2c5-c89ab7cdf731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Using device: cpu\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0/140, Loss: 0.5944, Acc: 81.25%\n",
      "Batch 50/140, Loss: 0.2514, Acc: 86.64%\n",
      "Batch 100/140, Loss: 0.3383, Acc: 90.10%\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create attention mask for current batch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# mask = #TODO\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# your code here\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# output = #TODO\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# your code here\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main training script\n",
    "# Load dataset from CSV file\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('spam_dataset.csv')\n",
    "\n",
    "# Convert spam/ham labels to binary values (0/1)\n",
    "df['Category'] = (df['Category'] == 'spam').astype(int)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Message'].values, df['Category'].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize text preprocessor with vocabulary and sequence length limits\n",
    "print(\"Preprocessing data...\")\n",
    "preprocessor = TextPreprocessor(max_vocab_size=10000, max_seq_length=128)\n",
    "preprocessor.build_vocab(train_texts)\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "train_dataset = SpamDataset(train_texts, train_labels, preprocessor)\n",
    "val_dataset = SpamDataset(val_texts, val_labels, preprocessor)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Set up device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the transformer model - you can experiment with different number of attention heads\n",
    "# You can also change the number of layers or the max sequence length to see how that affects performance\n",
    "model = SpamTransformer(\n",
    "    vocab_size=len(preprocessor.word2idx),\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=2,\n",
    "    max_seq_length=128\n",
    ").to(device)\n",
    "\n",
    "# Set up training parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize lists to track losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Main training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Visualize training progress\n",
    "plot_losses(train_losses, val_losses)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), 'spam_transformer.pth')\n",
    "print(\"Model saved to spam_transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "veFI1b8lbNc7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28491492a473819e40a552c2bdbf7004",
     "grade": false,
     "grade_id": "cell-8dd9b9f85e1735f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Questions to think about and future exercises\n",
    "\n",
    "- Is your validation loss rising again?\n",
    "- How would you avoid overfitting through early stopping?\n",
    "- Write a prediction function (run one forward pass through the transformer) to see how well your model predicts the output for an example sentence of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4ehhXGQWaBJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQWaS7GdJ8yrYf1aDZhWPd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
