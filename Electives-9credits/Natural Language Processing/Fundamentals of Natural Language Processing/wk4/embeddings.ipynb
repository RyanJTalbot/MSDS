{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15861fe39bed1c1940e214c7026eaed1",
     "grade": false,
     "grade_id": "cell-08c97dd804cb479a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Vector Space Semantics and Word Embeddings\n",
    "\n",
    "Word embeddings are a powerful concept in natural language processing (NLP) that involve representing words as vectors in a continuous vector space. Word embeddings capture semantic information about words. This means that words with similar meanings are represented by vectors that are close to each other in the vector space. \n",
    "\n",
    "In this assignment, we'll look at more traditional, *sparse* representations of words (i.e. where words are represented by vectors of size |V| with the word count corresponding to the word's index in the vocabulary), and then play with more advanced, *dense* representations of words, learned via neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae0c86d5b7ec567fa9980654c951dccd",
     "grade": false,
     "grade_id": "cell-711d2df7c8e73f79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Sparse Vector Representations\n",
    "\n",
    "We'll implement TF-IDF and PPMI, two methods to represent words and documents in a sparse matrix. We have provided a *term-document matrix* for the BLT corpus, treating each review as a document. The documents (columns) are represented via word count (rows). The columns of the matrix represent each review, and the rows represent every word in the vocabulary. One cell of the matrix gives the term frequency for that document.\n",
    "\n",
    "Because most words do not appear in most reviews, the matrix will be mostly 0s. This is why we call it sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc3794be3015639a7264cfc496a0c16b",
     "grade": false,
     "grade_id": "cell-fb9993afec7df9a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2611692113bb1d3d2314ca1db19a0205",
     "grade": false,
     "grade_id": "cell-08c84d9cd1774fe0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the term x doc matrix of the BLT corpus\n",
    "# Note: the matrix does not include reviews that were flagged for rejection in the corpus\n",
    "termxdoc = pd.read_csv('blt_termxdoc.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termxdoc # checking it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our term x doc matrix provides a representation of the BLT corpus in terms of the \n",
    "# frequency of each term in each document. The rows represent the terms and the columns \n",
    "# represent the documents. The values in the matrix represent the frequency of each term \n",
    "# in each document.\n",
    "#\n",
    "# The matrix is sparse, as most terms do not appear in most documents.\n",
    "# For example, let's find the frequency of the term 'grill' in each document:\n",
    "termxdoc.loc['grill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing across axis 1 (columns) gives the term frequencies across the entire corpus\n",
    "termxdoc.sum(axis=1).sort_values(ascending=False).head(10) # Top 10 terms by frequency; 'the' is the most frequent term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TF-IDF\n",
    "\n",
    "Let's implement TF-IDF using the term-document matrix. As a reminder, TF-IDF is a $|V| x N$ matrix in which the weighted value $w_{t,d}$ for word $t$ in document $d$ combines term frequency $\\text{tf}_{t,d}$ with the inverse document frequency $\\text{idf}$:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\text{tf}_{t,d} * \\text{idf}_t\n",
    "$$\n",
    "\n",
    "We will use logarithmically scaled term frequency:\n",
    "\n",
    "$$\n",
    "\\text{tf}_{t,d} = \\begin{cases}\n",
    "                     1 +\\log\\text{count}(t,d) & \\text{if count}(t,d) > 0 \\\\\n",
    "                     0 & \\text{otherwise}\n",
    "                   \\end{cases}\n",
    "$$\n",
    "\n",
    "The $\\text{idf}$ is defined using the fraction $N/\\text{df}_t$, where $N$ is the total number of documents in the collection, and $\\text{df}_t$ is the number of documents in which term $t$ occurs. The fewer documents in which a term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that occur in all the documents.\n",
    "\n",
    "We'll also logarithmically scale the inverse document frequency in our implementation:\n",
    "\n",
    "$$\n",
    "\\text{idf}_t = \\log \\frac{N}{\\text{df}_t} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8749b61a2c3fbcd8094cfc9698d43b6c",
     "grade": false,
     "grade_id": "cell-54d0f275eb39dbb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(termxdoc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"This function takes a raw term x doc matrix and returns a term x doc matrix with tfidf values.\n",
    "    Remember, the rows are the term counts and the columns are the documents.\"\"\"\n",
    "    \n",
    "    # Get the term frequency\n",
    "    log_counts = np.log(termxdoc)\n",
    "    log_counts[np.isinf(log_counts)] = 0.0 # log(0) goes to 0\n",
    "    tfs = 1 + log_counts\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97301caf4d56864eee71bdd054c97f78",
     "grade": false,
     "grade_id": "cell-a65ca425074f841e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "blt_tfidf = tfidf(termxdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_tfidf.loc['grill'] # Check to see how the tfidf values for the term 'grill' have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a57b94ecf72a73a14f44b0ef1a5517e3",
     "grade": true,
     "grade_id": "cell-d11e1ab6e7a8110b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert blt_tfidf.loc['grill'][0] == 10.725269122772522\n",
    "print('TF-IDF implementation seems to be working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement PPMI\n",
    "\n",
    "In NLP, the pointwise mutual information between a target word $i$ and a context word $j$ is defined as the observations of $i$ and $j$ co-occurring, divided by our expectations of $i$ and $j$ occurring assuming they each occurred independently:\n",
    "\n",
    "$$\n",
    "PMI(i, j) = \\log\\frac{\\text{observed}(i, j)}{\\text{expected}(i, j)} = \\log\\frac{P(i, j)}{P(i)P(j)}\n",
    "$$\n",
    "\n",
    "Given a word co-occurrence matrix $W$ where $w_{i,j}$ gives the number of times word $w_i$ occurs with context $w_j$, we have\n",
    "\n",
    "$$\n",
    "\\text{observed}(i, j) = W_{i,j}\n",
    "$$\n",
    "\n",
    "and our $\\text{expected}(i,j)$ value can be defined as:\n",
    "\n",
    "$$\n",
    "\\text{expected}(i,j) = \\frac{\\text{rowsum}(i)\\cdot\\text{colsum}(j)}{\\text{sum}(W)}\n",
    "$$\n",
    "\n",
    "Finally, we define *positive* PMI (PPMI) as follows:\n",
    "\n",
    "$$\n",
    "PPMI_{ij} = \\max(\\log\\frac{\\text{observed}(i,j)}{\\text{expected}(i,j)},0)\n",
    "$$\n",
    "\n",
    "\n",
    "PPMI fixes the problem of taking the log of 0-count cells.\n",
    "\n",
    "TF-IDF measured term-document frequencies, but PPMI measures term-term frequencies. So, we've provided a term-term matrix for the BLT corpus. The counts are simple: for the matrix $W_{i,j}$, a context word $w_j$ appears in the same review as target word $w_i$ a total of $w_{i,j}$ times. To keep things relatively faster, we've truncated the matrix to the most frequent 3,000 terms (on both axes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61a8acba36f964acebf9cc96b5e0cc37",
     "grade": false,
     "grade_id": "cell-89959c2669b9c9b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the term x term matrix\n",
    "termxterm = pd.read_csv('blt_termxterm.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termxterm.loc['grill'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51da99992edd2518df08d8f65ba30ec7",
     "grade": false,
     "grade_id": "cell-d4db62b291524a89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ppmi(termxterm: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"This function takes a raw term x term matrix and returns a term x term matrix with PPMI values.\"\"\"\n",
    "    context_word_sums = termxterm.sum(axis=0) # sum context words (column sum)\n",
    "    sum_W = context_word_sums.sum() # total words in corpus\n",
    "    target_word_sums = termxterm.sum(axis=1) # sum target words (row sum)\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e439b6cde61f201c8fc8b9bbb6a57827",
     "grade": false,
     "grade_id": "cell-0e5341ec18c18cfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "blt_ppmi = ppmi(termxterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_ppmi.loc['grill'].sort_values(ascending=False) # Check to see how the PPMI values for the term 'grill' have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4987601666b7bb8b2ab823ffc3b8700b",
     "grade": true,
     "grade_id": "cell-cc02aad2064d9cd4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert blt_ppmi.loc['grill'][0] == 0.0\n",
    "print('PPMI implementation seems to be working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement nearest neighbors\n",
    "\n",
    "Now that we have a couple of different vector spaces for the BLT corpus, let's play around with them.\n",
    "\n",
    "Implement a `nearest_neighbors` function that returns the `k` nearest neighbors for a term, given a vector space (matrix). Use cosine similarity for the distance function:\n",
    "\n",
    "$$\n",
    "\\text{cosine distance}(u,v) = 1 - \\frac{u \\cdot v}{||u||_2 \\cdot ||v||_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae2663c3ab94fda9c0e07d0f202fbb2f",
     "grade": false,
     "grade_id": "cell-7472d6f0116051c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(term: str, matrix: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    \"\"\"This function takes a term, a term x doc matrix, and a number of neighbors k, \n",
    "    and returns the k nearest neighbors of the term using cosine distance.\"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matrix = pd.DataFrame(\n",
    "    [[1.0,  2.0],\n",
    "    [2.0, 1.0],\n",
    "    [11.0, 20.0],\n",
    "    [18.0, 1.0]],\n",
    "    index=['a', 'b', 'c', 'd'],\n",
    "    columns=['x', 'y'])\n",
    "sample_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c99563203917513429fdab1e04047bfb",
     "grade": true,
     "grade_id": "cell-db33b27b30ed70c4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('a', sample_matrix, 1).index\n",
    "assert 'c' in neighbors_to_check\n",
    "neighbors_to_check = nearest_neighbors('a', sample_matrix, 2).index\n",
    "assert 'b' in neighbors_to_check\n",
    "print('Nearest neighbors implementation is working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors('grill', blt_tfidf, 10) # Get the 10 nearest neighbors of the term 'grill' using the tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30ae5c699290b2e9347b91d8999ad462",
     "grade": true,
     "grade_id": "cell-ab6a3f0bfe64728f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('grill', blt_tfidf, 10).index\n",
    "nns_tfidf = ['george', 'foreman', 'steaks', 'college', 'roommates', 'coming', 'usability', 'deceiving', 'hum', 'humid']\n",
    "for nn in nns_tfidf:\n",
    "    assert nn in neighbors_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors('grill', blt_ppmi, 10) # Get the 10 nearest neighbors of the term 'grill' using the PPMI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17847778a1fbaa7c54c0c4067692ba10",
     "grade": true,
     "grade_id": "cell-3fc54f2389dc4c3c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('grill', blt_ppmi, 10).index\n",
    "nns_ppmi = ['george', 'foreman', 'plants', 'forty', 'steaks', 'bear', 'peninsula', 'freshly', 'housekeepers', 'culture']\n",
    "for nn in nns_ppmi:\n",
    "    assert nn in neighbors_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dimensionality reduction\n",
    "\n",
    "We can capture latent relations among the words in our vocabulary words by reducing the dimensions of our sparse matrices and getting dense embeddings. We'll explore a technique called Latent Semantic Analysis (LSA) to do this with our term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LSA\n",
    "\n",
    "LSA uses SVD (singular value decomposition) to factor a matrix into three matrices:\n",
    "\n",
    "$$\n",
    "X_{mxn} = T_{mxr} S_{rxr} V^T_{rxn}\n",
    "$$\n",
    "\n",
    "$S$ is diagonal matrix containing what are called the singular values. The entries in $S$ along the diagonal are non-negative values sorted in descending order.\n",
    "\n",
    "For LSA, for a term-document matrix $X_{mxn}$ in which we have $m$ terms and $n$ documents, we keep only the $k$ highest values in $S$, along with the first $k$ columns of $T$:\n",
    "\n",
    "$$\n",
    "LSA(X_{mxn}) = T_{mxk} S_{kxk}\n",
    "$$\n",
    "\n",
    "(We can throw away $V^T_{rxn}$).\n",
    "\n",
    "\n",
    "Feel free to use a library for SVD, e.g. https://numpy.org/doc/2.1/reference/generated/numpy.linalg.svd.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8f47211ecbf7df1f7099ad39c51e020",
     "grade": false,
     "grade_id": "cell-7b6ee379d6b3a5d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def lsa(matrix: pd.DataFrame, k: int):\n",
    "    \"\"\"This function takes a term x doc matrix and a number of components k, and returns the LSA matrix.\"\"\"\n",
    "    T, S, V_T = np.linalg.svd(matrix, full_matrices=False)\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return lsa_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d173704e08b360fec3b50a377013e235",
     "grade": false,
     "grade_id": "cell-bf90cc2a7811f2da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes...\n",
    "blt_lsa_50 = lsa(termxdoc, 50) # Get the LSA matrix with 50 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2462977672efc3e4544f00a65b3a5d05",
     "grade": false,
     "grade_id": "cell-7bfb0a1ce627fbb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This one will take a while too...\n",
    "blt_lsa_100 = lsa(termxdoc, 100) # Get the LSA matrix with 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "993f4215060dc62bf518f3d14b13b63c",
     "grade": false,
     "grade_id": "cell-dca235f9b2523f09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And this one will take even longer. Hang in there!\n",
    "blt_lsa_300 = lsa(termxdoc, 300) # Get the LSA matrix with 300 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_lsa_100 # Our term x doc matrix is not so sparse anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_lsa_100.loc['grill'] # The 'grill' term vector in the LSA matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the nearest neighbors of the term 'grill' in our LSA matrices\n",
    "nearest_neighbors('grill', blt_lsa_50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5aae0adcaf1e975ab29c6a765155b17",
     "grade": true,
     "grade_id": "cell-5c73b9fbf43aef4f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('grill', blt_lsa_50, 10).index\n",
    "nns_lsa50 = ['george', 'foreman', 'boxing', 'consummate', 'instantaneously', 'slapped', 'stupendous', 'champ', 'unto', 'steaks']\n",
    "for nn in nns_lsa50:\n",
    "    assert nn in neighbors_to_check\n",
    "print('LSA (n=50)) implementation seems to be working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors('grill', blt_lsa_100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce0fa240ee499bce10680cb87bd0e572",
     "grade": true,
     "grade_id": "cell-e019ab8921cb71f9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('grill', blt_lsa_100, 10).index\n",
    "nns_lsa100 = ['george', 'foreman', 'boxing', 'consummate', 'instantaneously', 'slapped', 'stupendous', 'champ', 'unto', 'steaks']\n",
    "for nn in nns_lsa100:\n",
    "    assert nn in neighbors_to_check\n",
    "print('LSA (n=100)) implementation seems to be working!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors('grill', blt_lsa_300, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82691124c4dbc7bd91764f9d5a0f6689",
     "grade": true,
     "grade_id": "cell-d624d234fc81866c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors_to_check = nearest_neighbors('grill', blt_lsa_300, 10).index\n",
    "nns_lsa300 = ['george', 'foreman', 'instantaneously', 'slapped', 'consummate', 'boxing', 'stupendous', 'scare', 'unto', 'champ']\n",
    "for nn in nns_lsa300:\n",
    "    assert nn in neighbors_to_check\n",
    "print('LSA (n=300)) implementation seems to be working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a019ea2f335b503d71eb90ec7501d7c",
     "grade": false,
     "grade_id": "cell-80cf0330a658e15e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Sentiment Analysis\n",
    "\n",
    "Let's see how we do on sentiment analysis on the BLT corpus using our LSA matrices. We can think of our LSA matrices as 50-, 100-, and 300-dimension word embeddings. Similar to our ngram language modeling approach to sentiment analysis, we can try to classify the sentiment of unseen reviews by finding reviews that are similar to known positive reviews or known negative reviews.\n",
    "\n",
    "We'll represent our positive and negative reviews using our LSA word embeddings, and then, given an unseen review, we'll take a vote: compare it across all of the reviews, and classify it as positive if the majority of the similar reviews are positive, and classify it as negative if the majority of the similar reviews are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a6623bf5d46ad1914b67345a87021c9",
     "grade": false,
     "grade_id": "cell-35947410bbf6bbda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representing reviews with LSA embeddings\n",
    "\n",
    "Let's create a function `review2vec` that takes the text of a review and a matrix of embeddings, and turns it into a vector based on those embeddings.\n",
    "\n",
    "We will tokenize the review and then find word embeddings for every word in the review. Then, we'll take the centroid of all the embeddings in the review: that is to say, we'll simply average all the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8395cf59e0ffd1b77e4a1286d3fa46a3",
     "grade": false,
     "grade_id": "cell-2fe457b47c2af3af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def review2vec(review: str, matrix: pd.DataFrame) -> np.array:\n",
    "    \"\"\"This function takes a review and a matrix of word embeddings, \n",
    "    and returns the vector representation of the review.\n",
    "    The review vector is the average of the vectors of the \n",
    "    words in the review that are in the matrix.\"\"\"\n",
    "    # Get words in review\n",
    "    # We're tokenizing the review using the same tokenizer used to build the term x doc matrix\n",
    "    # This avoids out-of-vocabulary words\n",
    "    words = vectorizer.build_tokenizer()(review)\n",
    "\n",
    "    # Initialize review vector as zeros\n",
    "    review_vector = np.zeros(matrix.shape[1])\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return review_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b85ae3cd54bc9f839226523b2920e17",
     "grade": true,
     "grade_id": "cell-6bb0d5917647457f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vector_to_check = review2vec(\"This is a fake review\", blt_lsa_50)\n",
    "assert vector_to_check[0] == -18.86910966512524\n",
    "print(\"Review2vec implementation seems to be working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec5838c698f1f1139dd71216995c8766",
     "grade": false,
     "grade_id": "cell-50e9ff62c44a9312",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's test our review2vec technique for sentiment classification\n",
    "# First we need to read in all the reviews and their sentiment labels\n",
    "# This may take a minute or two\n",
    "import csv\n",
    "data_and_annotations = {\n",
    "    'train': {\n",
    "        'pos': {'ids': [], 'vectors': {50: [], 100: [], 300: []}},\n",
    "        'neg': {'ids': [], 'vectors': {50: [], 100: [], 300: []}}\n",
    "    },\n",
    "    'test': {'ids': [], 'sentiment': [], 'vectors': {50: [], 100: [], 300: []}}\n",
    "}\n",
    "with open('positive_reviews_train.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        id_ = row[0]\n",
    "        review = row[1]\n",
    "        for n in [50, 100, 300]:\n",
    "            model = blt_lsa_50 if n == 50 else blt_lsa_100 if n == 100 else blt_lsa_300\n",
    "            review_vector = review2vec(review, model)\n",
    "            data_and_annotations['train']['pos']['vectors'][n].append(review_vector)\n",
    "        data_and_annotations['train']['pos']['ids'].append(id_)\n",
    "with open('negative_reviews_train.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        id_ = row[0]\n",
    "        review = row[1]\n",
    "        for n in [50, 100, 300]:\n",
    "            review_vector = review2vec(review, globals()[f'blt_lsa_{n}'])\n",
    "            data_and_annotations['train']['neg']['vectors'][n].append(review_vector)\n",
    "        data_and_annotations['train']['neg']['ids'].append(id_)\n",
    "with open('test.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        sentiment = row[2]\n",
    "        review = row[1]\n",
    "        id_ = row[0]\n",
    "        for n in [50, 100, 300]:\n",
    "            review_vector = review2vec(review, globals()[f'blt_lsa_{n}'])\n",
    "            data_and_annotations['test']['vectors'][n].append(review_vector)\n",
    "        data_and_annotations['test']['ids'].append(id_)\n",
    "        data_and_annotations['test']['sentiment'].append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector representation of the first positive review\n",
    "data_and_annotations['train']['pos']['ids'][0], data_and_annotations['train']['pos']['vectors'][100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b28cd6462c313e4aa79bb17d0c0b27ca",
     "grade": false,
     "grade_id": "cell-89fba498d00227d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(\n",
    "    review_vector: np.array, \n",
    "    positive_review_vectors: list, \n",
    "    negative_review_vectors: list, \n",
    "    k: int = 10) -> str:\n",
    "    \"\"\"This function takes a review vector and two lists of positive and negative review vectors, \n",
    "    and returns the sentiment of the review.\n",
    "    The sentiment is determined by the vote of the k nearest neighbors.\"\"\"\n",
    "    # Calculate the cosine similarity between the review vector and the positive and negative review vectors\n",
    "    positive_sim = np.dot(positive_review_vectors, review_vector) / ((np.linalg.norm(positive_review_vectors, axis=1) * np.linalg.norm(review_vector)))\n",
    "    negative_sim = np.dot(negative_review_vectors, review_vector) / ((np.linalg.norm(negative_review_vectors, axis=1) * np.linalg.norm(review_vector)))\n",
    "    # Find top k nearest neighbors\n",
    "    topk_positive = positive_sim.argsort()[-k:][::-1]\n",
    "    topk_negative = negative_sim.argsort()[-k:][::-1]\n",
    "    # Get the vote of the top k most similar reviews\n",
    "    vote = sum(positive_sim[topk_positive] > negative_sim[topk_negative])\n",
    "    # Return the sentiment of the review\n",
    "    if vote > k/2:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7b3bfa227c7288eaca20c725597200d",
     "grade": false,
     "grade_id": "cell-967b5a00aff4b514",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's define experimental conditions\n",
    "ks = [1, 3, 5, 7, 10]\n",
    "n_components = [50, 100, 300]\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1290c533371be893ce4014f9c1df0fbd",
     "grade": false,
     "grade_id": "cell-41c8d1adb2b28007",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This may take a few minutes as well!\n",
    "from copy import deepcopy\n",
    "test_ids = data_and_annotations['test']['ids']\n",
    "test_sentiment = data_and_annotations['test']['sentiment']\n",
    "for n in n_components:\n",
    "    print('Running experiments for LSA with {} components...'.format(n))\n",
    "    results[n] = {}\n",
    "    test_review_vectors = deepcopy(data_and_annotations['test']['vectors'][n])\n",
    "    positive_review_vectors = deepcopy(data_and_annotations['train']['pos']['vectors'][n])\n",
    "    negative_review_vectors = deepcopy(data_and_annotations['train']['neg']['vectors'][n])\n",
    "\n",
    "    for k in ks:\n",
    "        print('Running experiments for k = {}...'.format(k))\n",
    "        results[n][k] = {}\n",
    "        \n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "\n",
    "        for t_i, t_v, t_s in zip(test_ids, test_review_vectors, test_sentiment):\n",
    "            predicted_sentiment = classify_sentiment(t_v, positive_review_vectors, negative_review_vectors, k)\n",
    "            if predicted_sentiment == 'pos':\n",
    "                if t_s == 'pos':\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                if t_s == 'neg':\n",
    "                    tn += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n",
    "\n",
    "        results[n][k]['accuracy'] = accuracy\n",
    "        results[n][k]['precision'] = precision\n",
    "        results[n][k]['recall'] = recall\n",
    "        results[n][k]['f1'] = f1\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8942b8ded59088fd9e1682b011384683",
     "grade": true,
     "grade_id": "cell-a107e3171faad2c4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for n in n_components:\n",
    "    for k in ks:\n",
    "        fscore = results[n][k]['f1']\n",
    "        assert fscore > .6\n",
    "print('The method seems to be working as it should!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ks_50 = list(results[50].keys())\n",
    "ks_100 = list(results[100].keys())\n",
    "ks_300 = list(results[300].keys())\n",
    "\n",
    "f1s_50 = [results[50][k]['f1'] for k in ks]\n",
    "f1s_100 = [results[100][k]['f1'] for k in ks]\n",
    "f1s_300 = [results[300][k]['f1'] for k in ks]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ks_50, f1s_50, marker='o', label='n = 50')\n",
    "plt.plot(ks_100, f1s_100, marker='o', label='n = 100')\n",
    "plt.plot(ks_300, f1s_300, marker='o', label='n = 300')\n",
    "plt.xticks(ks)\n",
    "plt.title('LSA with n components')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32caba7da41ffca1a8d91bd56ec1a032",
     "grade": false,
     "grade_id": "cell-46c0a89b11c35c9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representing reviews with GloVe embeddings\n",
    "\n",
    "Now, let's do the exact same thing, but use pre-trained embeddings. We'll use 100-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings, but feel free to explore independently using embeddings from other sources.\n",
    "\n",
    "How do you think our LSA vectors of the same size (n=100) will compare to GloVe? What about our larger (n=300) LSA vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "414b07504ea34904b847346b6fda1d20",
     "grade": false,
     "grade_id": "cell-b8742fee0bca0610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get pre-trained embeddings\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['grill'] # just taking a peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba3b0815221e87018c9c91c759544f3",
     "grade": false,
     "grade_id": "cell-b4a661460dad8731",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's put the glove embeddings into a pd.DataFrame for compatibility with our previous functions\n",
    "glove_matrix = pd.DataFrame({word: word_vectors[word] for word in word_vectors.key_to_index.keys()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix.loc['grill'] # The 'grill' term vector in the GloVe matrix - looks like our LSA embeddings now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b476cf6fb759fcfd4d8935d2b2f1d7",
     "grade": false,
     "grade_id": "cell-5cf2f212e69f5601",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's redo our experiments with GloVe embeddings\n",
    "data_and_annotations['train']['pos']['glove'] = []\n",
    "data_and_annotations['train']['neg']['glove'] = []\n",
    "data_and_annotations['test']['glove'] = []\n",
    "with open('positive_reviews_train.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        review = row[1]\n",
    "        glove_review_vector = review2vec(review, glove_matrix)\n",
    "        data_and_annotations['train']['pos']['glove'].append(glove_review_vector)\n",
    "with open('negative_reviews_train.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        review = row[1]\n",
    "        glove_review_vector = review2vec(review, glove_matrix)\n",
    "        data_and_annotations['train']['neg']['glove'].append(glove_review_vector)\n",
    "with open('test.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        review = row[1]\n",
    "        glove_review_vector = review2vec(review, glove_matrix)\n",
    "        data_and_annotations['test']['glove'].append(glove_review_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define experimental conditions\n",
    "ks = [1, 3, 5, 7, 10]\n",
    "results['glove-100'] = {}\n",
    "\n",
    "print('Running experiments for GloVe embeddings...')\n",
    "test_ids = data_and_annotations['test']['ids']\n",
    "test_sentiment = data_and_annotations['test']['sentiment']\n",
    "\n",
    "test_review_vectors = deepcopy(data_and_annotations['test']['glove'])\n",
    "positive_review_vectors = deepcopy(data_and_annotations['train']['pos']['glove'])\n",
    "negative_review_vectors = deepcopy(data_and_annotations['train']['neg']['glove'])\n",
    "\n",
    "for k in ks:\n",
    "    print('Running experiments for k = {}...'.format(k))\n",
    "    results['glove-100'][k] = {}\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for t_i, t_v, t_s in zip(test_ids, test_review_vectors, test_sentiment):\n",
    "        predicted_sentiment = classify_sentiment(t_v, positive_review_vectors, negative_review_vectors, k)\n",
    "        if predicted_sentiment == 'pos':\n",
    "            if t_s == 'pos':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if t_s == 'neg':\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n",
    "\n",
    "    results['glove-100'][k]['accuracy'] = accuracy\n",
    "    results['glove-100'][k]['precision'] = precision\n",
    "    results['glove-100'][k]['recall'] = recall\n",
    "    results['glove-100'][k]['f1'] = f1\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results, along with the LSA results\n",
    "ks_glove = list(results['glove-100'].keys())\n",
    "f1s_glove = [results['glove-100'][k]['f1'] for k in ks]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ks_50, f1s_50, marker='o', label='LSA n = 50')\n",
    "plt.plot(ks_100, f1s_100, marker='o', label='LSA n = 100')\n",
    "plt.plot(ks_300, f1s_300, marker='o', label='LSA n = 300')\n",
    "plt.plot(ks_glove, f1s_glove, marker='o', label='GloVe n = 100')\n",
    "plt.xticks(ks)\n",
    "plt.title('LSA vs GloVe embeddings')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c3bbd9438c15bfd10a82445e480111a",
     "grade": false,
     "grade_id": "cell-ac268bd396539578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Looks like our LSA embeddings couldn't keep up with the pre-trained GloVe embeddings. To close out the homework, take some time to reflect and maybe speculate on why that would be: is it the way GloVe was trained? Or maybe it's the data GloVe was trained on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
