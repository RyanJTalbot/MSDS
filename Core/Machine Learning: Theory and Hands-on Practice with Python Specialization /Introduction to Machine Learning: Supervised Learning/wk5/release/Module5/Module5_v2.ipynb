{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48cc68d5e6ef4d62dd331e0a56d2fe65",
     "grade": false,
     "grade_id": "cell-453482616cdb8d13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Grading\n",
    "The final score that you will receive for your programming assignment is generated in relation to the total points set in your programming assignment itemâ€”not the total point value in the nbgrader notebook.<br>\n",
    "When calculating the final score shown to learners, the programming assignment takes the percentage of earned points vs. the total points provided by nbgrader and returns a score matching the equivalent percentage of the point value for the programming assignment.<br>\n",
    "**DO NOT CHANGE VARIABLE OR METHOD SIGNATURES** The autograder will not work properly if your change the variable or method signatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69b74d87abe50685b53f0f0f8dc6f392",
     "grade": false,
     "grade_id": "cell-d4dddd5e9272f333",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### WARNING\n",
    "Please refrain from using **print statements/anything that dumps large outputs(>500 lines) to STDOUT** to avoid running to into **memory issues**. \n",
    "Doing so requires your entire lab to be reset which may also result in loss of progress and you will be required to reach out to Coursera for assistance with this.\n",
    "This process usually takes time causing delays to your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9d15ae6e4f88b3533cb8014739fd933",
     "grade": false,
     "grade_id": "cell-5a2bfb186865cf89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Validate Button\n",
    "Please note that this assignment uses nbgrader to facilitate grading. You will see a **validate button** at the top of your Jupyter notebook. If you hit this button, it will run tests cases for the lab that aren't hidden. It is good to use the validate button before submitting the lab. Do know that the labs in the course contain hidden test cases. The validate button will not let you know whether these test cases pass. After submitting your lab, you can see more information about these hidden test cases in the Grader Output. <br>\n",
    "***Cells with longer execution times will cause the validate button to time out and freeze. Please know that if you run into Validate time-outs, it will not affect the final submission grading.*** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "858db041ca18418851312ee5acb5ea89",
     "grade": false,
     "grade_id": "cell-3c0f1d3b9e739cf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 5: Ensemble methods (adaBoost, random forests) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0d6749181b5e355f06c07b09992bbe",
     "grade": false,
     "grade_id": "cell-57691d3f0f3d712f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "305ea6860ac6210e8de96e45a2d08ee2",
     "grade": false,
     "grade_id": "cell-2cb4f68eef534010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the helper code below to create a training and validation set of threes and eights from the MNIST dataset. There is also a helper function to display digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8280522ca34851557005286ac66e105",
     "grade": false,
     "grade_id": "cell-42b1785ca793847d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ThreesandEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST 3s and 8s data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        x_train, y_train, x_test, y_test = pickle.load(f)\n",
    "                \n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.x_train = x_train[np.logical_or(y_train== 3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or(y_train== 3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.x_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.x_train = self.x_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.x_test = x_test[np.logical_or(y_test== 3, y_test == 8), :]\n",
    "        self.y_test = y_test[np.logical_or(y_test== 3, y_test == 8)]\n",
    "        self.y_test = np.array([1 if y == 8 else -1 for y in self.y_test])\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "def view_digit(ex, label=None, feature=None):\n",
    "    \"\"\"\n",
    "    function to plot digit examples \n",
    "    \"\"\"\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    img = ex.reshape(21,21)\n",
    "    col = np.dstack((img, img, img))\n",
    "    if feature is not None: col[feature[0]//21, feature[0]%21, :] = [1, 0, 0]\n",
    "    plt.imshow(col)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "data = ThreesandEights(\"data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f204445656ac10cff884e71e35c9c46",
     "grade": false,
     "grade_id": "cell-64e3366069583de8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1: Building an Adaboost Classifier to classify MNIST digits 3 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f38766fb07c738cccba991bb22a02b3",
     "grade": false,
     "grade_id": "cell-f492e6362be1f32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "927bd4f00ea70315d7e2b9f947ab82b3",
     "grade": false,
     "grade_id": "cell-d8f15295dfa1942a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=3), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "        # you don't have to use sklearn's fit function, \n",
    "        # but it is probably the easiest way \n",
    "\n",
    "        # w = np.ones(len(y_train))\n",
    "        # h = clone(self.base)\n",
    "        # h.fit(X_train, y_train, sample_weight=w)\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return self  \n",
    "            \n",
    "    def error_rate(self, y_true, y_pred, weights):\n",
    "        # =================================================================\n",
    "        # TODO \n",
    "\n",
    "        # Implement the weighted error rate\n",
    "        # =================================================================\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # =================================================================\n",
    "        # TODO\n",
    "        # =================================================================\n",
    "        yhat = np.zeros(X.shape[0])\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        return np.array(scores)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6f0dfe27f6997c4b82310f1e2bc3d80",
     "grade": false,
     "grade_id": "cell-f4fce861f3b2900d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A: [Peer Review, 5 pts]** In the `AdaBoost` class above, implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "Recall that the AdaBoost algorithm is as follows: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) Fit kth weak learner to training data with weights w`\n",
    "\n",
    "$~~~~~~~$ `    b) Computed weighted error errk for the kth weak learner` \n",
    "\n",
    "$~~~~~~~$ `    c) compute vote weight alpha[k] = 0.5 ln ((1-errk)/errk))`\n",
    "\n",
    "$~~~~~~~$ `    d) update training example weights w[i] *= exp[-alpha[k] y[i] h[k](x[i])]`\n",
    "\n",
    "$~~~~~~~$ `    e) normalize training weights so they sum to 1`\n",
    "<br>\n",
    "For this week's Peer Review assignment, you wil upload a screenshot of the fit and error_rate functions from the AdaBoost class. Then you will upload a screenshot of using the fit function to fit the Adaboost classifier with 150 base decision stumps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbce3cfc36a48b0c0b6de5563e33c340",
     "grade": false,
     "grade_id": "cell-4f4acabb25dd62f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the fit function to fit the Adaboost classifier with 150 base decision tree stumps. [5 pts, Peer Review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e79fd796b5a7611e0454384623d22c3d",
     "grade": false,
     "grade_id": "cell-7ab85ec815746434",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use fit function to fit Adaboost classifier called clf with 150 base decision stumps\n",
    "# your code here\n",
    "\n",
    "# clf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "751ec316c546d48d585fcc6d3bb46649",
     "grade": true,
     "grade_id": "cell-b1710d6b7de2374c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests using the fit function to fit AdaBoost classifier with 150 base decision stumps  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93afebff53c09067a029b980cb1b8aa2",
     "grade": false,
     "grade_id": "cell-da0bb0f15f3bce3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B [5 pts]:** After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples. You can test out the predictions in the cell below.   **Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "Just print out your predictions on the training set in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79a6b4f01b4fb64ee65b93fd94d86701",
     "grade": false,
     "grade_id": "cell-4fdff85a8c9e6389",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# print out predictions on the training set \n",
    "# your code here\n",
    "\n",
    "# train_predict = \n",
    "print(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d643e34a282ebb4f79f5c77caca5d4",
     "grade": true,
     "grade_id": "cell-56582ae44c7e8a04",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests train_predict which uses the predict method  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf83b9274482dde17c38aa4d65abff24",
     "grade": false,
     "grade_id": "cell-925fb725ff149f43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C [Peer Review]:** Once your `predict` function is written up, you need to test the scores on the function. To do this compute the scores on the prediction in the `score` function. Use the `score` function to then complete `staged_score` to collect the scores for every boosting iterations. Plot the misclassification error for train and test sets (misclassification error = 1- score). <br>\n",
    "**Note:** your code for this section may cause the Validate button to time out. If you want to run the Validate button prior to submitting, you could comment out the code in this section after completing the Peer Review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfc23be1a573f90b7f049ffe870af2af",
     "grade": false,
     "grade_id": "cell-a3f37d45515375fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot misclassification error for train and test sets \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0c4eb1ddbe65707e8652c1e37a2512b",
     "grade": false,
     "grade_id": "cell-c878ca4627ff8331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2 [ 5 pts, Peer Review] : Building an Random Forest Classifier to classify MNIST digits 3 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0cbf8227db738b9b33d5829c5b0537f",
     "grade": false,
     "grade_id": "cell-13dff04654655660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember that training the random forest algorithms involves the following steps: \n",
    "\n",
    "`for k=1 to K:`\n",
    "\n",
    "$~~~~~~~$ `    a) build kth tree of depth d `\n",
    "\n",
    "$~~~~~~~$ `    b) Return the kth tree trained on the subset of dataset with the random feature splits`\n",
    "\n",
    "Predicting the classification result on new data involves returning the majority vote by all the trees in the random forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1d0b838c63b5361acd732b495da08c1",
     "grade": false,
     "grade_id": "cell-fbc92bf11a3d06c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A [5 points, Peer Review]:** Complete the `create_tree` function to build a new tree trained on a subset of data. Within this function a decision tree classifier is built and trained on the subset of data with the subset of features. Answer the Peer Review question for this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bb469d648e093d61609115416748177",
     "grade": false,
     "grade_id": "cell-2a9fac3a8f7c3fcf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, x, y, sample_sz, n_trees=200, n_features='sqrt', max_depth=10, min_samples_leaf=5):\n",
    "        \"\"\"\n",
    "        Create a new random forest classifier.\n",
    "        \n",
    "        Args:\n",
    "            x : Input Feature vector\n",
    "            y : Corresponding Labels\n",
    "            sample_sz : Sample size\n",
    "            n_trees : Number of trees to ensemble\n",
    "            n_features : Method to select subset of features \n",
    "            max_depth : Maximum depth of the trees in the ensemble\n",
    "            min_sample_leaf : Minimum number of samples per leaf \n",
    "        \"\"\"\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])  \n",
    "        self.features_set = []\n",
    "        self.x, self.y, self.sample_sz, self.max_depth, self.min_samples_leaf  = x, y, sample_sz, max_depth, min_samples_leaf\n",
    "        self.trees = [self.create_tree(i) for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self,i):\n",
    "        \"\"\"\n",
    "        create a single decision tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        idxs = np.asarray(idxs)\n",
    "\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        f_idxs = np.asarray(f_idxs)\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            self.features_set = np.array(f_idxs, ndmin=2)\n",
    "        else:\n",
    "            self.features_set = np.append(self.features_set, np.array(f_idxs,ndmin=2),axis=0)\n",
    "        \n",
    "        # TODO: build a decision tree classifier and train it with x and y that is a subset of data (use idxs and f_idxs)\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        # clf = \n",
    "        # x, y = \n",
    "       \n",
    "    def predict(self, x):\n",
    "        \n",
    "        # TODO: create a vector of predictions  and return\n",
    "        # You will have to return the predictions of the final ensembles based on the individual trees' predicitons\n",
    "        \n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # TODO: Compute the score using the predict function and true labels y\n",
    "        \n",
    "        # your code here\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "121dc7f807d5eac84d75c95d6fbc196c",
     "grade": true,
     "grade_id": "cell-9ee26343a1c4f51c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests create_tree function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2c5b415a2041f9aebcce6bc70b5921a",
     "grade": false,
     "grade_id": "cell-bd35978978ba4fc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B [Peer Review]:** In this part you will have to complete three steps: \n",
    "\n",
    "1. Complete the `predict` function in RandomForest class so as to make predictions using just the features. \n",
    "2. Finally complete the RandomForest class by completing the `score`function to compute the random forest model's accuracy on any dataset. \n",
    "3. Build a random forest classifier and train it on the MNIST data to classify 3s and 8s in the cell below. Then see how the classifier performs on the test data by computing the misclassification error. (Remember: error = 1-score)\n",
    "<br>\n",
    "Answer the Peer Review questions about this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d385a6251746488f0497bb2634a1da8d",
     "grade": false,
     "grade_id": "cell-2d182b373095f48e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: build a random forest classifier and make predictions\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print('Misclassification error on test data : %0.3f'%pred_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
